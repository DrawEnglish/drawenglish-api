import os, json, re
import spacy
import uvicorn
from fastapi import FastAPI
from fastapi.responses import JSONResponse, FileResponse  # renderì— 10ë¶„ ë‹¨ìœ„ Ping ë³´ë‚´ê¸°ë¥¼ ìœ„í•´ ì¶”ê°€
from pydantic import BaseModel
# ì•„ë˜ api_key= ê¹Œì§€ëŠ” .env íŒŒì¼ì—ì„œ OpenAIí‚¤ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° ê´€ë ¨ ë¶€ë¶„ 
from openai import OpenAI
from dotenv import load_dotenv

# â— í™˜ê²½ ì„¤ì •
load_dotenv()

api_key = os.getenv("API_KEY") or os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("âŒ OPENAI_API_KEY is not set in environment variables.")
client = OpenAI(api_key=api_key)

app = FastAPI()  # FastAPI() ê°ì²´ë¥¼ ìƒì„±í•´ì„œ ì´í›„ ë¼ìš°íŒ…ì— ì‚¬ìš©

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸°, ì—†ìœ¼ë©´ 'en_core_web_sm' ê¸°ë³¸ê°’
model_name = os.getenv("SPACY_MODEL", "en_core_web_sm")

try:
    nlp = spacy.load(model_name)
except OSError:
    # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ í›„ ë‹¤ì‹œ ë¡œë“œ
    from spacy.cli import download
    download(model_name)
    nlp = spacy.load(model_name)

# â— ì‹¬ë³¼ ë§¤í•‘
role_to_symbol = {
    "verb": "â—¯",
    "object": "â–¡",
    "indirect object": "â–¡",
    "direct object": "â–¡",
    "prepositional object": "â–¡",
    "preposition": "â–½",
    "conjunction": "â—‡",
    "noun subject complement": "[",
    "adjective subject complement": "(",
    "noun object complement": "[",
    "adjective object complement": "("
}

verb_attr_symbol = {
    "present tense": "|",
    "past tense": ">",
    "perfect aspect": "P",
    "progressive aspect": "i",
    "passive voice": "^",
    "subjunctive mood": "ã€‹"
}

verbals_symbol = {
    "R": "@",          # bare infinitive or root
    "to R": "to@",     # to infinitive
    "R-ing": "@ing",   # gerund or present participle
    "R-ed": "@ed"      # past participle
}

relative_words_symbol = {
    "relative pronoun": "[X]",
    "relative adjective": "(_)",
    "relative adverb": "<_>",
    "compound relative pronoun": "[e]",
    "compound relative adjective": "(e)",
    "compound relative adverb": "<e>",
    "interrogative pronoun": "[?]",
    "interrogative adjective": "(?)",
    "interrogative adverb": "<?>"
}

# ì „ì²´ ì‹¬ë³¼ í†µí•© ë”•ì…”ë„ˆë¦¬
symbols = {
    "role": role_to_symbol,
    "verb_attr": verb_attr_symbol,
    "verbals": verbals_symbol,
    "relatives": relative_words_symbol
}

# â— ë©”ëª¨ë¦¬ êµ¬ì¡° / ë©”ëª¨ë¦¬ ì´ˆê¸°í™”í™”
memory = {
    "symbols_by_level": {},
    "symbols": symbols
}


# level ë°œìƒ íŠ¸ë¦¬ê±° dep ëª©ë¡ (ì „ì—­ìœ¼ë¡œ í†µì¼)
level_trigger_deps = [
    "relcl", "acl", "advcl", "advmodcl", "ccomp", "xcomp", "csubj", "parataxis"
]

# ğŸ“˜ ë³´ì–´ê°€ **ëª…ì‚¬ë§Œ** ê°€ëŠ¥í•œ SVOC ë™ì‚¬
SVOC_noun_only = {
    "name", "appoint", "elect", "dub", "label", "christen", "nominate"
}

# ğŸ“™ ë³´ì–´ê°€ **í˜•ìš©ì‚¬ë§Œ** ê°€ëŠ¥í•œ SVOC ë™ì‚¬
SVOC_adj_only = {
    "find", "keep", "leave", "consider"  # ì¼ë¶€ ë³´ì–´ ëª…ì‚¬ë„ ê°€ëŠ¥í•˜ê¸´ í•˜ì§€ë§Œ ê±°ì˜ í˜•ìš©ì‚¬ ìš°ìœ„
}

# ğŸ“— ë³´ì–´ê°€ **ëª…ì‚¬/í˜•ìš©ì‚¬ ë‘˜ ë‹¤** ê°€ëŠ¥í•œ SVOC ë™ì‚¬
SVOC_both = {
    "make", "call", "consider", "declare", "paint", "think", "judge"
}

noSubjectComplementVerbs = {
    "live", "arrive", "go", "come", "sleep", "die", "run", "walk", "travel", "exist", "happen"
}

noObjectVerbs = {
    "die", "arrive", "exist", "go", "come", "vanish", "fall", "sleep", "occur"
}

modalVerbs_present = {"will", "shall", "can", "may", "must"}
modalVerbs_past = {"would", "should", "could", "might"}
modalVerbs_all = modalVerbs_present | modalVerbs_past

beVerbs = {"be", "am", "are", "is", "was", "were", "been", "being"}

notbeLinkingVerbs_onlySVC = {
    "become", "come", "go", "fall", "sound", "look", "smell", "taste", "seem"
}
notbeLinkingVerbs_SVCSVO = {
    "get", "turn", "grow", "feel"
}
netbeLinkingVerbs_all = notbeLinkingVerbs_SVCSVO | notbeLinkingVerbs_onlySVC

dativeVerbs = {
    "give", "send", "offer", "show", "lend", "teach", "tell", "write", "read", "promise",
    "sell", "pay", "pass", "bring", "buy", "ask", "award", "grant", "feed", "hand", "leave", "save", 
    "bring", "bake", "build", "cook", "sing", "make"  # dative verbë¡œ ì‚¬ìš© ë“œë¬¸ ê²ƒë“¤
}


# spaCyê°€ ì „ì¹˜ì‚¬ë¡œ ì˜¤ì¸ íƒœê¹…í•˜ëŠ” íŠ¹ìˆ˜ ë‹¨ì–´ë“¤
blacklist_preposition_words = {"due", "according"}


# â— ìš”ì²­/ì‘ë‹µ ëª©ë¡
class AnalyzeRequest(BaseModel):   # ì‚¬ìš©ìê°€ ë³´ë‚¼ ìš”ì²­(sentence) ì •ì˜
    sentence: str

class AnalyzeResponse(BaseModel):  # ì‘ë‹µìœ¼ë¡œ ëŒë ¤ì¤„ ë°ì´í„°(sentence, diagramming) ì •ì˜
    sentence: str
    diagramming: str               # "     â—‹______â–¡__[         "
    verb_attribute: dict

class ParseRequest(BaseModel):     # spaCy ê´€ë ¨ ì„¤ì •
    text: str


# rule ê¸°ë°˜ ë¶„ì„ ë¼ˆëŒ€ í•¨ìˆ˜ ì„ ì–¸
def rule_based_parse(tokens):
    result = []
    for t in tokens:
        t["children"] = [c["idx"] for c in tokens if c["head_idx"] == t["idx"]]
        t["role1"] = None
        t["role2"] = None

        # role ì¶”ë¡ 
        role1 = guess_role(t, tokens)
        if role1:
            t["role1"] = role1  # combineì—ì„œ ì“°ì¼ ìˆ˜ ìˆìŒ

    # 'name'ê³¼ ê°™ì€ ë™ì‚¬ê°€ ìˆëŠ” SVOCêµ¬ì¡°ì—ì„œ ëª©ì ë³´ì–´ë¥¼ ì˜ëª» íƒœê¹…í•˜ëŠ” ê²ƒ ë³´ì • í•¨ìˆ˜
    result = tokens  # ê¸°ì¡´ tokensì„ ìˆ˜ì •í•˜ë©° ê³„ì† ì‚¬ìš©
    result = assign_noun_complement_for_SVOC_noun_only(result)

    # âœ… ë³´ì–´ ê¸°ë°˜ object ë³µêµ¬ ìë™ ì ìš©
    result = repair_object_from_complement(result)        

    return result


# role ì¶”ë¡  í•¨ìˆ˜
def guess_role(t, all_tokens=None):  # all_tokens ì¶”ê°€ í•„ìš”
    dep = t.get("dep")
    pos = t["pos"]
    head_idx = t.get("head_idx")

    # âœ… Subject
    if dep in ["nsubj", "nsubjpass"]:
        return "subject"

    # âœ… Main Verb: beë™ì‚¬ í¬í•¨, ì¢…ì†ì ˆë„ ê³ ë ¤
    if pos in ["VERB", "AUX"] and (dep in level_trigger_deps or dep == "root"):
        return "verb"

    # âœ… ë“±ìœ„ì ‘ì†ì‚¬ ë‹¤ìŒ ë³‘ë ¬ ë™ì‚¬ (conj)ë„ verb role ë¶€ì—¬
    if pos == "VERB" and dep == "conj":
        if any(t["idx"] == head_idx and t.get("role1") == "verb" for t in all_tokens):
            return "verb"


    # âœ… Indirect Object
    if dep in ["iobj", "dative"]:
        return "indirect object"

    # âœ… Direct Object (SVOO êµ¬ì¡° íŒë‹¨)
    if dep in ["dobj", "obj"]:
        if head_lemma := next((t["lemma"] for t in all_tokens if t["idx"] == head_idx), None):
            if head_lemma in noObjectVerbs:
                return None  # âŒ ëª©ì ì–´ ê¸ˆì§€ ë™ì‚¬ â†’ ë¬´ì‹œ

        # âœ… ê¸°ì¡´ object íŒë‹¨ ë¡œì§
        if all_tokens:
            for other in all_tokens:
                if other.get("dep") in ["iobj", "dative"] and other.get("head_idx") == head_idx:
                    return "direct object"
        return "object"


    # âœ… Preposition: ê¸°ë³¸ prep, agent + ë³´ì™„ (pcomp), ë‹¨ blacklist ë‹¨ì–´ëŠ” ì œì™¸
    if (
        (dep in ["prep", "agent"] or (dep == "pcomp" and pos == "ADP" and t.get("tag") == "IN"))
        and t["text"].lower() not in blacklist_preposition_words
    ):
        return "preposition"

    # âœ… Prepositional Object (by ~pobj êµ¬ì¡° ì»¤ë²„)
    # ìœ„ Preposition Roleê²°ì •ì‹œ blacklist ë‹¨ì–´ì— ì˜í•´ ì „ì¹˜ì‚¬ì˜ ëª©ì ì–´ë¥¼ ëª»ì°¾ëŠ” ë¬¸ì œ ë³´ì •
    if dep == "pobj":
        head_token = next((t for t in all_tokens if t["idx"] == head_idx), None)
        if head_token and (
            head_token.get("role1") == "preposition"
            or (
                head_token["text"].lower() in blacklist_preposition_words and
                (
                    head_token.get("pos") == "ADP" or
                    head_token.get("dep") == "prep" or
                    head_token.get("tag") == "IN"
                )
            )
        ):
            return "prepositional object"
    
    # âœ… Conjunction or Clause Marker (ì ‘ì†ì‚¬)
    if dep in ["cc", "mark"]:
        return "conjunction"

    # âœ… Subject Complement (SVC êµ¬ì¡°)
    if dep in ["attr", "acomp"]:
        if head_lemma := next((t["lemma"] for t in all_tokens if t["idx"] == head_idx), None):
            if head_lemma in noSubjectComplementVerbs:
                return None  # âŒ ë³´ì–´ ë¶ˆê°€ ë™ì‚¬ â†’ ì°¨ë‹¨

        if pos in ["NOUN", "PROPN", "PRON"]:
            return "noun subject complement"
        elif pos == "ADJ":
            return "adjective subject complement"

    # âœ… Object Complement (ì •ìƒ ì¼€ì´ìŠ¤: dep=oprd, xcomp)
    if dep in ["oprd", "xcomp", "ccomp"]:
        if pos in ["NOUN", "PROPN", "PRON"]:
            return "noun object complement"
        elif pos == "ADJ":
            return "adjective object complement"

    # âœ… Object Complement (ë³´ì™„ ì¼€ì´ìŠ¤: dep=advmod, pos=ADJ, head=VERB, dobj ì¡´ì¬ ì‹œ)
    if dep == "advmod" and pos == "ADJ":
        for tok in all_tokens:
            if tok["idx"] == head_idx and tok["pos"] == "VERB":
                obj_exists = any(
                    c.get("head_idx") == tok["idx"] and c.get("dep") in ["dobj", "obj"]
                    for c in all_tokens
                )
                if obj_exists:
                    return "adjective object complement"

    # âœ… ê·¸ ì™¸ëŠ” DrawEnglish ë„ì‹ì—ì„œ ì‚¬ìš© ì•ˆ í•¨
    return None

# depê°€ dativeì—¬ì„œ indiret objectê°€ ìˆëŠ”ë°, ë’¤ìª½ì— direct object roleì´ ì—†ëŠ” ê²½ìš° ë³´ì •
def recover_direct_object_from_indirect(parsed):
    """
    SVOO ë¬¸ì¥ì—ì„œ indirect objectì— ëŒ€í•´ appos êµ¬ì¡°ì˜ direct objectë¥¼ ë³µì›
    """
    for token in parsed:
        if token.get("role1") == "indirect object":
            token_idx = token.get("idx")

            for child in parsed:
                if (
                    child.get("head_idx") == token_idx and
                    child.get("dep") == "appos" and
                    child.get("pos") in {"NOUN", "PROPN"}
                ):
                    child["role1"] = "direct object"
                    break

    return parsed


# ëª©ì ë³´ì–´ë¡œ ëª…ì‚¬ë§Œ ì·¨í•˜ëŠ” ë™ì‚¬ ì‚¬ìš© ë¬¸ì¥ì—ì„œ ëª©ì ë³´ì–´ë¥¼ ì˜ëª» íƒœê¹…í•˜ëŠ” ê²ƒ ë³´ì •
# ê·¸ í›„ ì•„ë˜ repair_object_from_complement()í•¨ìˆ˜ë¥¼ í†µí•´ ëª©ì ì–´ë¥¼ ë³´ì •í•¨
def assign_noun_complement_for_SVOC_noun_only(parsed):
    """
    SVOC êµ¬ì¡° ë™ì‚¬ë“¤(SVOC_noun_only ì‚¬ì „ì— ë“±ë¡)ì˜ ëª©ì ë³´ì–´ê°€ spaCyì—ì„œ ì˜ëª» íƒœê¹…ëœ ê²½ìš°
    noun object complementë¡œ 1íšŒ ë³´ì • ë‹¨, object ì´í›„ì˜ ë‹¨ì–´ë§Œ ëŒ€ìƒìœ¼ë¡œ í•œë‹¤.
    """
    applied = False

    for i, token in enumerate(parsed):
        if token.get("lemma") in SVOC_noun_only and token.get("pos") in ["VERB", "AUX"]:
            verb_idx = token["idx"]

            # object í™•ì¸
            obj = next((t for t in parsed if t.get("head_idx") == verb_idx and t.get("dep") in ["dobj", "obj"]), None)
            if not obj:
                continue

            obj_idx = obj["idx"]

            # ë³´ì–´ í›„ë³´ ì°¾ê¸° : objedt ë’¤ì— ìˆëŠ” ëª…ì‚¬ì‚¬
            for t in parsed:
                if applied:
                    break

                if (
                    t.get("idx") > obj_idx and  # âœ… object ì´í›„ì— ë“±ì¥í•œ ë‹¨ì–´ë§Œ
                    t.get("head_idx") == verb_idx and
                    t.get("dep") in ["nsubj", "nmod", "attr", "appos", "npadvmod", "ccomp"] and
                    t.get("pos") in ["NOUN", "PROPN"]
                ):
                    t["role1"] = "noun object complement"
                    applied = True
                    break

    return parsed

# ëª©ì ë³´ì–´ë¡œ í˜•ìš©ì‚¬ë§Œ ë˜ëŠ” í˜•ìš©ì‚¬/ëª…ì‚¬ë¥¼ ëª¨ë‘ ì·¨í•˜ëŠ” ë™ì‚¬ ì‚¬ìš© ë¬¸ì¥ì—ì„œ ëª©ì ë³´ì–´ë¥¼ ì˜ëª» íƒœê¹…í•˜ëŠ” ê²ƒ ë³´ì •
# ê·¸ í›„ ì•„ë˜ repair_object_from_complement()í•¨ìˆ˜ë¥¼ í†µí•´ ëª©ì ì–´ë¥¼ ë³´ì •í•¨
# ì˜ˆ: "She painted the wall green."
def assign_adj_object_complement_when_compound_object(parsed):
    for verb in parsed:
        if verb.get("pos") != "VERB":
            continue

        verb_lemma = verb.get("lemma")
        if verb_lemma not in SVOC_adj_only and verb_lemma not in SVOC_both:
            continue

        verb_idx = verb["idx"]

        # ë³´ì–´ í›„ë³´: VERBì˜ ìì‹ ì¤‘ dep=obj, pos=ADJ
        for t in parsed:
            if (
                t.get("head_idx") == verb_idx and
                t.get("dep") in ["dobj", "obj"] and
                t.get("pos") == "ADJ"
            ):
                # âœ… ì´ ADJì˜ childrenì— compoundê°€ ë¶™ì–´ ìˆìœ¼ë©´ ë³´ì • ëŒ€ìƒ
                children = [c for c in parsed if c.get("head_idx") == t["idx"]]
                has_compound = any(
                    c.get("dep") == "compound" and c.get("pos") == "NOUN"
                    for c in children
                )
                if has_compound:
                    t["role1"] = "adjective object complement"

    return parsed

# SVOC_both ë™ì‚¬ì— ì†í•´ ìˆê³ , ë’¤ì— role(object)ê°€ ìˆê³ , ê·¸ ë’¤ì— advcl, ADJ ì´ë©´ì„œ HEADê°€ objectì™€ ê°™ì„ë•Œ
# adjective object complementë¡œ ë³´ì •.  ì˜ˆ: He painted the kitchen walls blue.
def assign_adj_complement_for_advcl_adjective(parsed):
    """
    spaCyê°€ í˜•ìš©ì‚¬ ëª©ì ë³´ì–´ë¥¼ advclë¡œ ì˜ëª» íƒœê¹…í–ˆì„ ë•Œ ë³´ì •
    ì˜ˆ: "He painted the walls blue."
    """
    for verb in parsed:
        if verb.get("pos") != "VERB":
            continue
        if verb.get("lemma") not in SVOC_both:
            continue

        verb_idx = verb["idx"]

        # 1. object ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
        obj = next(
            (t for t in parsed if t.get("head_idx") == verb_idx and t.get("role1") in ["object", "direct object"]),
            None
        )
        if not obj:
            continue

        obj_idx = obj["idx"]

        # 2. object ì´í›„ ë“±ì¥í•œ í˜•ìš©ì‚¬ ì¤‘ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ ë³´ì–´ë¡œ ê°„ì£¼
        for t in parsed:
            if (
                t.get("idx") > obj_idx and
                t.get("head_idx") == verb_idx and
                t.get("dep") == "advcl" and
                t.get("pos") == "ADJ"
            ):
                t["role1"] = "adjective object complement"
    return parsed


# ëª©ì ë³´ì–´(object complement)ê°€ ìˆëŠ”ë°, ì•ìª½ ëª©ì ì–´ë¥¼ nsubj(subject)ë¡œ ì˜ëª» íƒœê¹…í•˜ëŠ” ê²½ìš° ì˜ˆì™¸ì²˜ë¦¬
def repair_object_from_complement(parsed):
    for item in parsed:
        if item.get("role1") in ["noun object complement", "adjective object complement"]:
            complement_children = item.get("children", [])
            
            # âœ… ë™ì‚¬ ê¸°ì¤€ ê°€ì¥ ê°€ê¹Œìš´ compound ì¤‘ object í›„ë³´ í•„í„°ë§
            compound_candidates = [
                t for t in parsed 
                if (
                    t["idx"] in complement_children and
                    t.get("dep") == "compound" and
                    t.get("pos") == "NOUN" and
                    t.get("head_idx") == item["idx"]
                )
            ]
            
            if compound_candidates:
                # ğŸ”¹ ê°€ì¥ ë‚®ì€ idx (ë™ì‚¬ì— ê°€ê¹Œìš´ ë‹¨ì–´) ì„ íƒ
                compound_candidates.sort(key=lambda x: x["idx"])
                compound_candidates[0]["role1"] = "object"

            # ë³´ì™„: ì¢…ì¢… ì£¼ì–´ë¥¼ objectë¡œ ì˜ëª» ë„£ê¸°ë„ í•¨ (ì´ê±´ ê·¸ëŒ€ë¡œ ìœ ì§€)
            for t in parsed:
                if t.get("dep") == "nsubj" and t.get("idx") in complement_children:
                    t["role1"] = "object"

    return parsed


# combine ì¶”ë¡  í•¨ìˆ˜
def guess_combine(token, all_tokens):
    role1 = token.get("role1")
    idx = token.get("idx")
    combine = []

    # âœ… Verb â†’ object / complement (SVO, SVC)
    if role1 == "verb":
        for t in all_tokens:
            if (
                t.get("head_idx") == idx
                and t["idx"] > idx  # ğŸ”§ ì˜¤ë¥¸ìª½ ë°©í–¥ ì—°ê²°ë§Œ í—ˆìš©
            ):
                r = t.get("role1")
                if r in [
                    "object",
                    "indirect object",
                    "noun subject complement",
                    "adjective subject complement",
                    "noun object complement",
                    "adjective object complement"  # ğŸ”§ ë³´ì–´ë„ ì—°ê²°ë˜ê²Œ!
                ]:
                    combine.append({"text": t["text"], "role1": r, "idx": t["idx"]})
                    # âœ… ë³´ì™„: indirect objectê°€ ìì‹ ê°–ê³  ìˆìœ¼ë©´ ê·¸ ì¤‘ direct objectë„ ì—°ê²°
                    if r == "indirect object":
                        children = [c for c in all_tokens if c.get("head_idx") == t["idx"]]
                        for c in children:
                            if (
                                c.get("role1") in ["direct object", "object"]
                                and c["idx"] > t["idx"]  # ğŸ”§ í•µì‹¬ ì¶”ê°€
                            ):
                                combine.append({"text": c["text"], "role1": c["role1"], "idx": c["idx"]})

    # âœ… Indirect object â†’ direct object (SVOO êµ¬ì¡°)
    if role1 == "indirect object":
        for t in all_tokens:
            if (
                t.get("role1") in ["direct object"] and
                t.get("head_idx") == token.get("head_idx")
                and t["idx"] > token["idx"]  # ğŸ”§ ì˜¤ë¥¸ìª½ ë°©í–¥ë§Œ ì—°ê²°
            ):
                combine.append({"text": t["text"], "role1": "direct object", "idx": t["idx"]})

    # âœ… Object â†’ object complement (SVOC êµ¬ì¡°)
    if role1 == "object":
        for t in all_tokens:
            t_role = t.get("role1") or ""
            if "object complement" in t_role:
                if (
                    t.get("head_idx") == idx or
                    idx == t.get("head_idx")
                ):
                    combine.append({"text": t["text"], "role1": t["role1"], "idx": t["idx"]})
                    continue

                # ğŸ”¹ ì¶”ê°€ ì—°ê²° ì¡°ê±´: ë³´ì–´ê°€ objectë³´ë‹¤ ë’¤ì— ìˆê³ , headëŠ” ë™ì¼í•œ ë™ì‚¬
                if (
                    t["idx"] > idx and
                    t.get("dep") in {"advcl", "oprd", "xcomp", "ccomp"} and
                    t.get("pos") == "ADJ" and
                    t.get("head_idx") == token.get("head_idx")
                ):
                    combine.append({"text": t["text"], "role1": t["role1"], "idx": t["idx"]})

    # âœ… Preposition â†’ prepositional object
    if role1 == "preposition":
        for t in all_tokens:
            if t.get("head_idx") == idx and t.get("role1") == "prepositional object":
                combine.append({"text": t["text"], "role1": "prepositional object", "idx": t["idx"]})

        # 2ï¸âƒ£ ì˜ˆì™¸ ë³´ì •: headê°€ due/accordingì¸ë°, ì´ tokenì´ ê·¸ ë’¤ì˜ "to"ì¼ ê²½ìš°
    for t in all_tokens:
        if (
            t.get("role1") == "prepositional object" and
            t.get("head_idx") in [
                b["idx"] for b in all_tokens if b["text"].lower() in blacklist_preposition_words
            ]
        ):
            # ğŸ‘‰ head token
            head_idx = t.get("head_idx")
            head_token = next((tok for tok in all_tokens if tok["idx"] == head_idx), None)

            # âœ… head_token ë‹¤ìŒì—ì„œ "to" ì°¾ê¸°
            to_token = next(
                (
                    tok for tok in all_tokens
                    if tok["text"].lower() == "to" and tok["idx"] > head_token["idx"]
                ),
                None
            )

            # âœ… ì´ tokenì´ ê·¸ "to"ì¼ ê²½ìš°ë§Œ ì—°ê²°
            if to_token and to_token["idx"] == idx:
                combine.append({"text": t["text"], "role1": t["role1"], "idx": t["idx"]})

    # âœ… combine ìˆì„ ê²½ìš°ë§Œ ë°˜í™˜
    return combine if combine else None


def assign_level_triggers(parsed):
    """
    ì ˆ íŠ¸ë¦¬ê±°(dep in trigger_deps)ê°€ ê°ì§€ë˜ë©´,
    ì ˆì˜ ì‹œì‘ ìœ„ì¹˜ì— í•´ë‹¹í•˜ëŠ” í† í°ì— level = 0.5 ë¶€ì—¬

    - relcl, advcl, ccomp, xcomp: children ì¤‘ ê°€ì¥ ì•ì— ì˜¤ëŠ” í† í°
    - acl: íŠ¸ë¦¬ê±° ë‹¨ì–´ ìì‹ 
    """

    for token in parsed:
        if token["dep"] not in level_trigger_deps:
            continue

        if not is_valid_clause_trigger(token):
            continue

        dep = token["dep"]
        token_idx = token["idx"]

        if dep == "acl":
        # aclì€ í˜„ì¬ ë‹¨ì–´ê°€ ì ˆ ì‹œì‘ì„
            token["level"] = 0.5
            continue

        # childrenì€ ê°ì²´ê°€ í•„ìš”í•¨ (idx ë¦¬ìŠ¤íŠ¸ ì•„ë‹˜)
        children = [t for t in parsed if t["head_idx"] == token_idx and t["idx"] != token_idx]

        if children:
            # ì ˆ ë‚´ ê°€ì¥ ì•ì— ì˜¤ëŠ” í† í°ì„ íŠ¸ë¦¬ê±°ë¡œ íŒë‹¨
            first_token = min(children, key=lambda x: x["idx"])
            first_token["level"] = 0.5

    return parsed

def is_nounchunk_trigger(token):

    # ëª…ì‚¬ì ˆ ì²«ë‹¨ì–´ íŠ¸ë¦¬ê±° ì¡°ê±´ : SCONJ + mark + IN
    if token.get("pos") == "SCONJ" and token.get("dep") == "mark" and token.get("tag") == "IN":
        return True

    # to ë¶€ì •ì‚¬
    if (
        token.get("pos") == "PART" and token.get("dep") == "aux" and token.get("tag") == "TO" and
        token.get("lemma", "").lower() == "to"
    ):
        return True

    return False

def is_adverbchunk_trigger(token):

    # ë¶€ì‚¬ì ˆ ì²«ë‹¨ì–´ íŠ¸ë¦¬ê±° ì¡°ê±´ : SCONJ + mark/advmod + IN/WRB
    return (
        token.get("pos") == "SCONJ" and
        token.get("dep") in {"mark", "advmod"} and
        token.get("tag") in {"IN", "WRB"}
    )


def assign_chunk_role2(parsed):

    # ëª…ì‚¬ë©ì–´ë¦¬/ë¶€ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ role2ì— í•´ë‹¹ê°’ ë¶€ì—¬ 

    chunk_info_list = []

    # ê³„ì¸µë°œìƒìš”ì†Œ(level x.5ë‹¨ì–´)ë§Œ ì•„ë˜ ì†ŒìŠ¤ ì²˜ë¦¬
    for token in parsed:
        level = token.get("level")
        if not (isinstance(level, float) and level % 1 == 0.5):
            continue

        #ê³„ì¸µë°œìƒìš”ì†Œì˜ í—¤ë“œ ê°’ì´ ìˆìœ¼ë©´ ì•„ë˜ ì†ŒìŠ¤ ì²˜ë¦¬
        head_idx = token.get("head_idx")
        head_token = next((t for t in parsed if t["idx"] == head_idx), None)
        if not head_token:
            continue

        head_dep = head_token.get("dep")

        # ëª…ì‚¬ë©ì–´ë¦¬ íŒë‹¨ : ê³„ì¸µë°œìƒìš”ì†Œ í—¤ë“œì˜ depê°€(ccomp, xcomp) ì´ê³ , 
        # ê³„ì¸µë°œìƒìš”ì†Œê°€ is_nounchunk_triggerì— ê±¸ë¦¬ë©´,
        if head_dep in {"ccomp", "xcomp"} and is_nounchunk_trigger(token):

            # ê³„ì¸µë°œìƒìš”ì†Œì˜ headì˜ head ì°¾ê¸° headê°’ì´ ìˆìœ¼ë©´ ì•„ë˜ ì†ŒìŠ¤ ì²˜ë¦¬
            head2_idx = head_token.get("head_idx")
            head2_token = next((t for t in parsed if t["idx"] == head2_idx), None)
            if not head2_token:
                continue

            # ê³„ì¸µë°œìƒìš”ì†Œ headì˜ headì¸ ìƒìœ„ë™ì‚¬(head2) lemmaê°’ ì €ì¥
            head2_lemma = head2_token.get("lemma", "")

            # 1) ëª…ì‚¬ë©ì–´ë¦¬ í™•ì •í›„ ìƒìœ„ë™ì‚¬ê°€ beë™ì‚¬ì™€ LinkingVerbsì´ë©´ ë³´ì–´ í™•ì •
            # ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ì˜ role2ì— 'noun subject complement'(ëª…ì‚¬ì£¼ì–´ë³´ì–´)ê°’ ì €ì¥
            if head2_lemma in beVerbs or head2_lemma in notbeLinkingVerbs_onlySVC:
                token["role2"] = "noun subject complement"

            # 2) ìƒìœ„ë™ì‚¬ê°€ dativeVerbsì¼ë•Œ ìƒìœ„ë™ì‚¬level ë‹¨ì–´ë“¤ì˜ role1ì— objedct, indirect objectê°€ ìˆìœ¼ë©´
            # ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ì˜ role2ì— 'direct object'(ì§ì ‘ëª©ì ì–´)ê°’ ì €ì¥
            # ì•„ë‹ˆë©´ role2ì— 'object'(ëª©ì ì–´)ê°’ ì €ì¥
            elif head2_lemma in dativeVerbs:
                current_level = int(token.get("level", 0))  # 0.5 -> 0
                # í˜„ì¬ ë ˆë²¨ì˜ í† í°ë“¤
                level_tokens = [t for t in parsed if int(t.get("level", -1)) == current_level]
                has_obj_or_iobj = any(
                    t.get("role1") in {"object", "indirect object"} for t in level_tokens
                )
                if has_obj_or_iobj:
                    token["role2"] = "direct object"
                else:
                    token["role2"] = "object"

            # ì• ëª¨ë“  ì¡°ê±´ì— ì•ˆê±¸ë¦¬ë©´ ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ì˜ rele2ì— 'object'(ëª©ì ì–´) ê°’ ì €ì¥
            else:
                token["role2"] = "object"


        # ì£¼ì–´ ëª…ì‚¬ë©ì–´ë¦¬ í™•ì • : ë©ì–´ë¦¬ìš”ì†Œ ì²«ë‹¨ì–´ì˜ headì˜ depê°€ csubj, nsubj, nsubjpassì´ê³ ,
        # is_nounchunk_trigger() í•¨ìˆ˜ì— ê±¸ë¦¬ë©´ role2ì— 'chunk_subject'ê°’ ì…ë ¥
        if head_dep in {"csubj", "nsubj", "nsubjpass"} and is_nounchunk_trigger(token):
            token["role2"] = "chunk_subject"

        # ë¶€ì‚¬ë©ì–´ë¦¬ í™•ì • : ë©ì–´ë¦¬ìš”ì†Œ ì²«ë‹¨ì–´ì˜ headì˜ depê°€ advclì´ê³ ,
        # is_adverbchunk_trigger() í•¨ìˆ˜ì— ê±¸ë¦¬ë©´ role2ì— 'chunk_adverb_modifier'ê°’ ì…ë ¥
        if head_dep == "advcl" and is_adverbchunk_trigger(token):
            token["role2"] = "chunk_adverb_modifier"

        
        # âœ… ë©ì–´ë¦¬ ì •ë³´ ìˆ˜ì§‘ (ë í† í° ì°¾ê¸° + ì‹œì‘ í† í° info)
        children_tokens = [child for child in parsed if child.get("head_idx") == head_idx]
        children_tokens.append(head_token)
        if not children_tokens:
            continue

        children_tokens.sort(key=lambda x: x["idx"])
        end_token = children_tokens[-1]

        # ë í† í°ì´ êµ¬ë‘ì (. ! ?)ì´ë©´ ê·¸ ì• í† í° ì‚¬ìš©
        if (
            end_token.get("pos") == "PUNCT" and
            end_token.get("text") in {".", "!", "?"} and
            len(children_tokens) >= 2
        ):
            end_token = children_tokens[-2]

        end_idx = end_token.get("idx")
        end_text = end_token.get("text", "")
        end_idx_adjusted = end_idx + len(end_text) - 1

        first_level = int(token.get("level"))
        first_idx = token.get("idx")

        # ë©ì–´ë¦¬ ìœ í˜•ë³„ role2 ì‹¬ë³¼ ê²°ì •
        role2_to_symbol = {
            "object": "â–¡",
            "direct object": "â–¡",
            "noun subject complement": "[",
            # ğŸ”¥ ì•ìœ¼ë¡œ ì¶”ê°€ ê°€ëŠ¥:
            # "adjective subject complement": "(",
            # "chunk_subject": "[",
            # "chunk_adverb_modifier": "<",
        }

        role2 = token.get("role2")
        symbol = role2_to_symbol.get(role2)

        if symbol:
            chunk_info = {
                "first_idx": first_idx,
                "first_level": first_level,
                "symbol": symbol,
                "end_idx_adjusted": end_idx_adjusted,
            }
            chunk_info_list.append(chunk_info)

    return chunk_info_list


def apply_chunk_symbols_overwrite(chunk_info_list):
    """
    ìˆ˜ì§‘ëœ ë©ì–´ë¦¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ
    1) ë©ì–´ë¦¬ ëë‹¨ì–´ì— ] ì‹¬ë³¼
    2) ë©ì–´ë¦¬ ì²«ë‹¨ì–´ì— role2 ì‹¬ë³¼(â–¡, [ ë“±) ì°ê¸°
    """
    symbols_by_level = memory["symbols_by_level"]
    line_length = memory["sentence_length"]

    for chunk in chunk_info_list:
        first_idx = chunk["first_idx"]
        first_level = chunk["first_level"]
        symbol = chunk["symbol"]
        end_idx_adjusted = chunk["end_idx_adjusted"]

        line = symbols_by_level.setdefault(first_level, [" " for _ in range(line_length)])

        # 1) ì²«ë‹¨ì–´ì— role2 ì‹¬ë³¼ ì°ê¸°
        if 0 <= first_idx < len(line):
            line[first_idx] = symbol

        # 2) ëë‹¨ì–´ ëê¸€ìì— ] ì‹¬ë³¼ ì°ê¸°
        if 0 <= end_idx_adjusted < len(line):
            line[end_idx_adjusted] = "]"


def apply_chunk_function_symbol(parsed):
    """
    role2=chunk_subjectì¸ í† í°ì„ ê¸°ì¤€ìœ¼ë¡œ
    í•´ë‹¹ ì ˆ(start_idx ~ end_idx) ë²”ìœ„ì— [ ] ì‹¬ë³¼ ë¶€ì—¬
    """
    line_length = memory["sentence_length"]
    symbols_by_level = memory["symbols_by_level"]

    for token in parsed:
        role2 = token.get("role2")
        if not role2:
            continue

        level = token.get("level")
        if level is None:
            continue

        line = symbols_by_level.setdefault(int(level), [" " for _ in range(line_length)])

        start_idx = token["idx"]
        head_idx = token.get("head_idx")
        head_token = next((t for t in parsed if t["idx"] == head_idx), None)

        if not head_token:
            continue

        children_tokens = [child for child in parsed if child.get("head_idx") == head_idx]
        children_tokens.append(head_token)
        if not children_tokens:
            continue

        children_tokens.sort(key=lambda x: x["idx"])

        end_token = children_tokens[-1]

        if end_token.get("pos") == "PUNCT" and len(children_tokens) >= 2:
            end_token = children_tokens[-2]

        end_idx = end_token["idx"]
        end_idx_adjusted = end_idx + len(end_token["text"]) - 1

        # âœ… role2ì— ë”°ë¼ ì‹¬ë³¼ ë‹¤ë¥´ê²Œ
        if role2 == "chunk_subject":
            left, right = "[", "]"
        elif role2 == "chunk_adverb_modifier":
            left, right = "<", ">"
        else:
            continue

        if 0 <= start_idx < line_length:
            line[start_idx] = left
        if 0 <= end_idx_adjusted < line_length:
            line[end_idx_adjusted] = right


def NounChunk_combine_apply_to_upverb(parsed):
    """
    ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ role2ê°€ object / direct object / noun subject complementì¼ë•Œ
    ìƒìœ„ ë™ì‚¬ì˜ comnbinì— role2ë¥¼ ì…ë ¥í•´ì£¼ëŠ” í•¨ìˆ˜
    """
    for token in parsed:
        role2 = token.get("role2")
        # ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ì˜ role2ê°€ ì´ 3ê°œì¼ë•Œë§Œ ì•„ë˜ ì†ŒìŠ¤ ì²˜ë¦¬
        if role2 not in {"object", "direct object", "noun subject complement"}:
            continue

        # ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ì˜ head(ë³´í†µ ë™ì‚¬)ì˜ depê°€ ccomp(ì¢…ì†ì ‘ì†ì‚¬)ì¼ë•Œë§Œ ì•„ë˜ ì†ŒìŠ¤ ì²˜ë¦¬
        head_idx = token.get("head_idx")
        head_token = next((t for t in parsed if t["idx"] == head_idx), None)
        if not head_token or head_token.get("dep") not in {"ccomp", "xcomp"}:
            continue

        # ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ì˜ headì˜ head(ìƒìœ„ ë™ì‚¬ head2)ê°€ ìˆìœ¼ë©´ ì•„ë˜ ì†ŒìŠ¤ ì²˜ë¦¬ë¦¬
        head2_idx = head_token.get("head_idx")
        head2_token = next((t for t in parsed if t["idx"] == head2_idx), None)
        if not head2_token:
            continue
        if "combine" not in head2_token or not head2_token["combine"]:
            head2_token["combine"] = []

        # ğŸ”¥ ìƒìœ„ ë™ì‚¬ì˜ combineì— ìœ„ role2 3ê°œì¤‘ 1ê°œ(text, role2ê°’, idxê°’) ì…ë ¥
        head2_token["combine"].append({
            "text": token["text"],
            "role2": role2,
            "idx": token["idx"]
        })


def assign_level_ranges(parsed):
    """
    ì¢…ì†ì ˆì„ ë‹´ë‹¹í•˜ëŠ” dep (relcl, acl, advcl, ccomp, xcomp)ì— ë”°ë¼
    í•´ë‹¹ ì ˆ ë²”ìœ„ì— level ê°’ì„ ë¶€ì—¬í•œë‹¤.
    
    - relcl, advcl, ccomp, xcomp: í•´ë‹¹ í† í° + children â†’ ë²”ìœ„ ê³„ì‚°
    - acl: í•´ë‹¹ í† í°ë¶€í„° children í¬í•¨í•˜ì—¬ ë²”ìœ„ ê³„ì‚° (ìê¸°ìì‹ ì´ ì—°ê²°ì–´)
    
    ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ì— level=Noneì¸ í† í°ë“¤ì— ëŒ€í•´ level=0ì„ ë¶€ì—¬í•œë‹¤.
    """

    current_level = 1  # ì‹œì‘ì€ 1ë¶€í„° (0ì€ ìµœìƒìœ„ ì ˆìš©)

    for token in parsed:
        dep = token.get("dep")
        if dep not in level_trigger_deps:
            continue
        
        if not is_valid_clause_trigger(token):
            continue

        token_idx = token["idx"]
        clause_tokens = [token]  # ì‹œì‘ì€ ìê¸° ìì‹  í¬í•¨

        # âœ… childrenë„ ì ˆ ë²”ìœ„ì— í¬í•¨
        children = [t for t in parsed if t["head_idx"] == token_idx]
        clause_tokens.extend(children)

        # âœ… ì ˆ ë²”ìœ„ ì‹œì‘ ~ ë ê³„ì‚°
        start_idx = min(t["idx"] for t in clause_tokens)
        end_idx = max(t["idx"] for t in clause_tokens)

        # âœ… level ë¶€ì—¬
        for t in parsed:
            if start_idx <= t["idx"] <= end_idx:
                t["level"] = current_level

        # âœ… ì—°ê²°ì–´ì—ëŠ” .5 ì¶”ê°€
        if dep == "acl":
            token["level"] = current_level - 0.5  # ì—°ê²°ì–´ëŠ” ë°”ë¡œ ì´ì „ ì ˆì—ì„œ ì´ì–´ì§
        else:
            # ì—°ê²°ì–´ í›„ë³´: ì ˆ ë²”ìœ„ ì• ë‹¨ì–´ ì¤‘ ì—°ê²°ì‚¬ ì—­í• 
            connector = min(clause_tokens, key=lambda x: x["idx"])
            connector["level"] = current_level - 0.5

        current_level += 1

    # âœ… ìµœìƒìœ„ ì ˆ level=None â†’ level=0 ìœ¼ë¡œ ì„¤ì •
    for t in parsed:
        if t.get("level") is None:
            t["level"] = 0

    return parsed

# ëª©ì ë³´ì–´ë¥¼ ê³„ì¸µ ìœ ë°œ ìš”ì†Œë¡œ íƒœê¹…í•´ levelì´ ë°œìƒí•˜ëŠ” ê²ƒì— ëŒ€í•œ ì˜ˆì™¸ì²˜ë¦¬ í•¨ìˆ˜ì„
def is_valid_clause_trigger(token: dict) -> bool:
    """
    ì ˆ(clause) íŠ¸ë¦¬ê±°ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í† í°ì¸ì§€ íŒë³„í•©ë‹ˆë‹¤.

    ì˜ˆì™¸ ì¡°ê±´:
    - depê°€ clauseìš© dep (ccomp, xcomp, advcl ë“±)ê°€ ì•„ë‹˜
    - ë³´ì–´ ì—­í• ì¸ ê²½ìš° (object complement ë“±)
    í–¥í›„ ì¡°ê±´ì´ ë” ìƒê¸°ë©´ ì—¬ê¸°ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    dep = token.get("dep")
    role1 = token.get("role1")
    pos = token.get("pos") 

    if dep not in level_trigger_deps:
        return False

    if role1 in ["adjective object complement", "noun object complement"]:
        return False

    # âœ… ì˜ˆì™¸: ADJì¸ë° dep=advcl ì¸ ê²½ìš°ëŠ” ì ˆ ì•„ë‹˜
    if dep == "advcl" and pos == "ADJ":
        return False
    
    # í–¥í›„ ë” ì˜ˆì™¸ì¡°ê±´ì´ ìƒê¸°ë©´ ì—¬ê¸°ì— ì¶”ê°€
    return True

def repair_level_within_prepositional_phrases(parsed):
    """
    ì „ì¹˜ì‚¬(prep ë˜ëŠ” agent)ì˜ ëª©ì ì–´(pobj) ë ˆë²¨ì´ ë‹¤ë¥¼ ê²½ìš°
    ì „ì¹˜ì‚¬ì˜ level ê¸°ì¤€ìœ¼ë¡œ ë²”ìœ„ ë‚´ í† í°ë“¤ì„ ë³´ì •.
    """

    for prep in parsed:
        if prep.get("dep") not in {"prep", "agent"}:
            continue

        prep_level = prep.get("level")
        if prep_level is None:
            continue

        prep_idx = prep["idx"]

        # âœ… ëª¨ë“  í† í° ì¤‘ì—ì„œ pobj í›„ë³´ ì°¾ê¸° (children ì¡°ê±´ ì œì™¸)
        pobj_candidates = [
            t for t in parsed
            if t.get("dep") == "pobj" and t.get("head_idx") == prep_idx
        ]

        for pobj in pobj_candidates:
            pobj_level = pobj.get("level")

            if pobj_level == prep_level:
                continue  # ì´ë¯¸ ë™ì¼í•˜ë©´ ê±´ë„ˆëœ€

            # âœ… prep ~ pobj ì‚¬ì´ ë²”ìœ„ë¥¼ ì°¾ì•„ level ë³´ì •
            start = min(prep_idx, pobj["idx"])
            end = max(prep_idx, pobj["idx"])

            for t in parsed:
                if start <= t["idx"] <= end:
                    t["level"] = prep_level
                    t["level_corrected_from_prep"] = True  # ë””ë²„ê¹…ìš© í‘œì‹œ

    return parsed


# ì•„ë¬´ ì‹¬ë³¼ë„ ì•ˆ ì°íŒ ì¤„ì´ë©´ memoryì—ì„œ ì•„ì˜ˆ ì œê±°
def clean_empty_symbol_lines():
    """
    memory["symbols_by_level"] ì¤‘ ë‚´ìš©ì´ ì „ë¶€ ê³µë°±ì¸ ì¤„ì€ ì œê±°í•œë‹¤.
    """
    keys_to_remove = []
    for level, line in memory["symbols_by_level"].items():
        if all(c == " " for c in line):
            keys_to_remove.append(level)

    for level in keys_to_remove:
        del memory["symbols_by_level"][level]


# ë™ì‚¬ë©ì–´ë¦¬(verb chain) í•˜ë‚˜ ë°›ì•„ì„œ ì‹œì œ/ìƒ/íƒœ ë¶„ì„í•˜ê³  symbol_map ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
def set_verbchunk_attributes(chain):

    symbol_map = {}
    aspect = []
    voice = None

    if not chain:
        return symbol_map, aspect, voice

    verb_attr = memory["symbols"]["verb_attr"]

    # ë§¨ ì• í† í°
    first = chain[0]
    first_lemma = first.get("lemma", "").lower()
    first_pos = first.get("pos")
    first_dep = first.get("dep")
    first_tag = first.get("tag")

    # âœ… P1. ë§¨ì• modal ì—¬ë¶€
    if first_pos == "AUX" and first_dep == "aux" and first_tag == "MD":
        if first_lemma in modalVerbs_present:
            symbol_map[first["idx"]] = verb_attr["present tense"]
        elif first_lemma in modalVerbs_past:
            symbol_map[first["idx"]] = verb_attr["past tense"]

    # âœ… P2. ì¤‘ê°„ ì¡°ë™ì‚¬ë“¤ ì²˜ë¦¬
    for t in chain:
        pos = t.get("pos")
        dep = t.get("dep")
        tag = t.get("tag")
        text = t.get("text", "").lower()

        # aux, auxpassë§Œ ì¡°ë™ì‚¬
        if not (pos == "AUX" and dep in {"aux", "auxpass"}):
            break  # ì¡°ë™ì‚¬ ì•„ë‹ˆë©´ (ì¦‰ ë³¸ë™ì‚¬) -> P3ë¡œ

        # ì¡°ë™ì‚¬ ì‹œì œ (fin)
        verbform = t.get("morph", {}).get("VerbForm", "")
        tense = t.get("morph", {}).get("Tense", "")

        if verbform == "Fin":
            if tag in {"VBP", "VBZ"} or tense == "Pres":
                symbol_map[t["idx"]] = verb_attr["present tense"]
            elif tag == "VBD" or tense == "Past":
                symbol_map[t["idx"]] = verb_attr["past tense"]

        # ì™„ë£Œ, ì§„í–‰
        if text == "been" and tag == "VBN":
            symbol_map[t["idx"]] = verb_attr["perfect aspect"]
            if "perfect" not in aspect:
                aspect.append("perfect")
        elif text == "being" and tag == "VBG":
            symbol_map[t["idx"]] = verb_attr["progressive aspect"]
            if "progressive" not in aspect:
                aspect.append("progressive")

        # ì›í˜• ì¡°ë™ì‚¬(VB, Inf)ëŠ” ì•„ë¬´ê²ƒë„ ì•ˆì°ê³  continue
        if tag == "VB" and verbform == "Inf":
            continue

    # âœ… P3. ë³¸ë™ì‚¬ ì²˜ë¦¬
    last = chain[-1]
    pos = last.get("pos")
    dep = last.get("dep")
    tag = last.get("tag")
    lemma = last.get("lemma", "").lower()

    if dep in level_trigger_deps or dep == "root":
        verbform = last.get("morph", {}).get("VerbForm", "")
        tense = last.get("morph", {}).get("Tense", "")

        if verbform == "Fin":
            if tag in {"VBP", "VBZ"} or tense == "Pres":
                symbol_map[last["idx"]] = verb_attr["present tense"]
            elif tag == "VBD" or tense == "Past":
                symbol_map[last["idx"]] = verb_attr["past tense"]


        # ì™„ë£Œ/ìˆ˜ë™/ì§„í–‰
        if tag == "VBN":
            # ì™¼ìª½ìœ¼ë¡œ AUX aux/auxpass ì°¾ì•„ì•¼ í•´
            for prev in reversed(chain[:-1]):
                if prev.get("pos") != "AUX":
                    continue
                prev_dep = prev.get("dep")
                prev_lemma = prev.get("lemma", "").lower()
                if prev_dep == "aux" and prev_lemma == "have":
                    symbol_map[last["idx"]] = verb_attr["perfect aspect"]
                    if "perfect" not in aspect:
                        aspect.append("perfect")
                    break
                elif prev_dep == "auxpass" and prev_lemma == "be":
                    symbol_map[last["idx"]] = verb_attr["passive voice"]
                    voice = "passive"
                    break

        elif tag == "VBG":
            symbol_map[last["idx"]] = verb_attr["progressive aspect"]
            if "progressive" not in aspect:
                aspect.append("progressive")

    return symbol_map, aspect, voice

# ë¬¸ì¥ì˜ ì „ì²´ parsed ê²°ê³¼ë¥¼ ë°›ì•„ ë™ì‚¬ë©ì–´ë¦¬ë³„ ì‹œì œ/ìƒ/íƒœ ë¶„ì„.
def set_allverbchunk_attributes(parsed):
    memory["verb_attribute_by_chain"] = []
    memory["verb_attribute"] = {}
    sentence_len = memory["sentence_length"]

    chains = []
    current_chain = []
    last_level = None

    # ë™ì‚¬ë©ì–´ë¦¬ ë¶„ë¦¬
    for token in parsed:
        level = token.get("level", 0)

        if last_level is None:
            last_level = level

        # ë“±ìœ„ì ‘ì†ì‚¬ê°€ ë‚˜ì˜¤ë©´ ë™ì‚¬ë©ì–´ë¦¬ ëŠìŒ (ì¢…ì†ì ‘ì†ì‚¬ëŠ” level ë°œìƒ ë¶€ë¶„ì—ì„œ ì²˜ë¦¬ ê°€ëŠ¥)
        if (
            token.get("dep") in {"cc"} and token.get("pos") in {"CCONJ", "CONJ"}
        ):
            if current_chain:
                chains.append(current_chain)
                current_chain = []
            last_level = level

        # level ë°”ë€” ë•Œ ëŠê¸°
        if last_level is not None and level != last_level:
            if current_chain:
                chains.append(current_chain)
                current_chain = []
            last_level = level

        # âœ… AUX, VERB ì¶”ê°€
        if token["pos"] in {"AUX", "VERB"}:
            current_chain.append(token)

    if current_chain:
        chains.append(current_chain)

    all_symbol_maps = {}

    # ê° chain ë¶„ì„
    for chain in chains:
        if not chain:
            continue
        
        first = chain[0]
        last = chain[-1]

        symbol_map, aspect, voice = set_verbchunk_attributes(chain)

        # ì €ì¥ (ë””ë²„ê¹…ìš©, í™•ì¥ìš©)
        memory["verb_attribute_by_chain"].append({
            "aspect": aspect,
            "voice": voice,
            "main_verb": chain[-1]["text"],
            "verb_chain": [t["text"] for t in chain],
            "symbol_map": symbol_map
        })

        all_symbol_maps.update(symbol_map)

    memory["verb_attribute"] = {
        "symbol_map": all_symbol_maps,
        "main_verb": last["text"],
        "aspect": aspect,
        "voice": voice
}

# â— GPT í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜
def spacy_parsing_backgpt(sentence: str, force_gpt: bool = False):
    doc = nlp(sentence)

    prompt = f"""

"""
    # spaCyì—ì„œ í† í° ë°ì´í„° ì¶”ì¶œ
    tokens = []
    for token in doc:
        morph = token.morph.to_dict()
        tokens.append({
            "idx": token.idx, "text": token.text, "pos": token.pos_,"tag": token.tag_,
            "dep": token.dep_.lower(), "head": token.head.text, "head_idx": token.head.idx,
            "tense": morph.get("Tense"), "aspect": morph.get("Aspect"), "voice": morph.get("Voice"), "form": morph.get("VerbForm"),
            "morph": morph, "lemma": token.lemma_, "is_stop": token.is_stop,
            "is_punct": token.is_punct, "is_alpha": token.is_alpha, "ent_type": token.ent_type_,
            "is_title": token.is_title, "children": [child.text for child in token.children]
        })

    # ê·œì¹™ ê¸°ë°˜ íŒŒì‹±
    parsed = rule_based_parse(tokens)

    # âœ… ë³´ì–´ í˜•ìš©ì‚¬ ë³´ì •: ADJì¸ë° objectë¡œ ëœ ê²½ìš°
    parsed = assign_adj_object_complement_when_compound_object(parsed)

    # âœ… ë³´ì–´ ê¸°ì¤€ìœ¼ë¡œ objectë¥¼ ë³µì› (compoundì¸ ê²½ìš° ë“±)
    parsed = repair_object_from_complement(parsed)

    # âœ… NEW: advcl+ADJ ë³´ì–´ ë³´ì •
    parsed = assign_adj_complement_for_advcl_adjective(parsed)

    # SVOO ê´€ë ¨ ë³´ì •(indirect object roleë§Œ ìˆëŠ” ê²½ìš°)
    parsed = recover_direct_object_from_indirect(parsed)


    # âœ… ìš”ê¸°! ëª¨ë“  ë³´ì • ëë‚œ í›„ì— combine ì¶”ë¡ 
    for t in parsed:
        combine = guess_combine(t, parsed)
        if combine:
            t["combine"] = combine

    # ì¡°ê±´: ê·œì¹™ ê¸°ë°˜ ì‹¤íŒ¨í•˜ê±°ë‚˜, ê°•ì œë¡œ GPT ì‚¬ìš© ìš”ì²­
    if not parsed or force_gpt:
        prompt = gpt_parsing_withprompt(tokens)  # ì•„ë˜ 2ë‹¨ê³„ì—ì„œ ë§Œë“¤ ì˜ˆì •

        try:
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are an expert sentence analyzer."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=500
            )
            content = response.choices[0].message.content
            return json.loads(content)
        except Exception as e:
            print("[ERROR] GPT parsing failed:", e)
            print("[RAW CONTENT]", content if 'content' in locals() else '[No response]')
            return []

    # âœ… ì ˆ ë¶„ê¸° íŠ¸ë¦¬ê±° ë¶€ì—¬ (0.5 level)
    parsed = assign_level_triggers(parsed)

    # level ë¶„ê¸° ì „íŒŒ
    parsed = assign_level_ranges(parsed)

    # âœ… ğŸ“ level ë³´ì •: prep-pobj ë ˆë²¨ í†µì¼
    parsed = repair_level_within_prepositional_phrases(parsed)

    set_allverbchunk_attributes(parsed)

    return parsed

# GPT API Parsing(with í”„ë¡¬í”„íŠ¸)ì„ ì´ìš©í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
def gpt_parsing_withprompt(tokens: list) -> str:
    token_lines = []
    for t in tokens:
        token_lines.append(
            f"â— idx({t['idx']}), text({t['text']}), pos({t['pos']}), tag({t['tag']}), dep({t['dep']}), head({t['head']})"
        )
    token_block = "\n".join(token_lines)

    prompt = f"""
Given the following tokenized and POS-tagged English sentence, analyze its syntactic structure.

Token Info:
{token_block}

Return a JSON list with each token's role in the sentence.
Each item must have: idx, text, role1, role2, and optionally combine/level.

If unsure, return best-guess. Do not return explanations, just the JSON.
"""
    return prompt.strip()


# â— ì €ì¥ê³µê°„ ì´ˆê¸°í™”
def init_memorys (sentence: str):
#    memory["characters"] = list(sentence)        # charactersì— sentenceì˜ ê¸€ì í•œê¸€ìì”© ì±„ìš°ê¸°
    memory["symbols_by_level"] = {}  # ë¬¸ì¥ë§ˆë‹¤ ìƒˆë¡œ ì´ˆê¸°í™”
    memory["sentence_length"] = len(sentence)  # ë„ì‹ ê¸¸ì´ ì¶”ì ìš© (ì¤„ ê¸¸ì´ í†µì¼)


# â— symbols ë©”ëª¨ë¦¬ì— ì‹¬ë³¼ë“¤ ì €ì¥í•˜ê¸°
def apply_symbols(parsed):
    symbols_by_level = memory["symbols_by_level"]
    line_length = memory["sentence_length"]

    for item in parsed:
        idx = item.get("idx", -1)
        role1 = str(item.get("role1", "") or "").lower()
        level = item.get("level")

        if idx < 0 or level is None:
            continue

        # âœ… 0.5ì²˜ëŸ¼ ê²½ê³„ ë ˆë²¨ì€ ë‘ ì¤„ì— ì‹¬ë³¼ ì°ê¸°
        levels = [level]
        if isinstance(level, float) and level % 1 == 0.5:
            levels = [int(level), int(level) + 1]

        # 
        symbol = role_to_symbol.get(role1)

        for lvl in levels:
            line = symbols_by_level.setdefault(lvl, [" " for _ in range(line_length)])
            if 0 <= idx < len(line) and line[idx] == " " and symbol:
                line[idx] = symbol

    # â¬‡ï¸ ì—¬ê¸°ì„œ combine ì—°ê²°ì„ ì„ _ ë¡œ ê·¸ë ¤ì¤Œ!
    for item in parsed:
        combine = item.get("combine")
        level = item.get("level")
        idx1 = item.get("idx")

        if not combine or level is None:
            continue

        for comb in combine:
            idx2 = comb.get("idx")  # âœ… text ë¹„êµ ëŒ€ì‹  idx ì§ì ‘ ì‚¬ìš©
            if idx2 is None:
                continue

            # ê°™ì€ ë ˆë²¨ ì¤„ì— ë°‘ì¤„ ì±„ìš°ê¸°
            lvl = int(level)  # levelì´ floatì´ë©´ intë¡œ ë³€í™˜

            line = symbols_by_level.setdefault(lvl, [" " for _ in range(line_length)])
            start = min(idx1, idx2)
            end = max(idx1, idx2)

            for i in range(start + 1, end):
                if line[i] == " ":
                    line[i] = "_"


# ì²˜ìŒ ë‚˜ì˜¤ëŠ” ì¡°ë™ì‚¬ì™€ ë³¸ë™ì‚¬ ì‚¬ì´ë¥¼ .(ì )ìœ¼ë¡œ ì—°ê²° ì‹œì¼œì¤Œ, ë ˆë²¨ ìˆœíšŒí•˜ë©°(ë‹¤ë¥¸ ë ˆë²¨ê°„ ì—°ê²°í• ì¼ ì—†ìŒ), ê¸°ì¡´ ë„í˜• ìˆìœ¼ë©´ ì•ˆì°ìŒ
def apply_aux_to_mverb_bridge_symbols_each_levels(parsed, sentence):

    for modal_token in [t for t in parsed if t["pos"] == "AUX" and t["dep"] in {"aux", "auxpass"}]:
        level = modal_token.get("level")
        if level is None:
            continue

        line = memory["symbols_by_level"].get(level)
        if not line:
            continue

        modal_idx = modal_token["idx"]

        # âœ… ì¡°ë™ì‚¬ ì´í›„ì— ë‚˜ì˜¤ëŠ” ì²« ë²ˆì§¸ ë³¸ë™ì‚¬(verb role)
        verb_token = next(
            (t for t in parsed
             if t.get("role1") == "verb"
             and t.get("level") == level
             and t["idx"] > modal_idx),
            None
        )
        if not verb_token:
            continue

        verb_idx = verb_token["idx"]
        start, end = sorted([modal_idx, verb_idx])

        # âœ… ì˜ë¬¸ë¬¸ íŒë‹¨
        has_subject_between = any(
            t.get("role1") == "subject" and start < t["idx"] < end
            for t in parsed
        )

        if has_subject_between:
            if line[modal_idx] == " ":
                line[modal_idx] = "âˆ©"
        else:
            if line[modal_idx] == " ":
                line[modal_idx] = "."

        for i in range(start + 1, end):
            if line[i] == " ":
                line[i] = "."


# ë™ì¼ë ˆë²¨, ê°™ì€ ì ˆì— ë™ì‚¬ê°€ ì—¬ëŸ¬ê°œ ë³‘ë ¬ ë‚˜ì—´ëœ ê²½ìš° ë™ì‚¬ë©ì–´ë¦¬ ì²˜ìŒ ìš”ì†Œì™€ ëìš”ì†Œë¥¼ .(ì )ìœ¼ë¡œ ì±„ì›Œì¤Œ
def draw_dot_bridge_across_verb_group(parsed):
    line_length = memory["sentence_length"]
    symbols_by_level = memory["symbols_by_level"]
    visited = set()

    for token in parsed:
        # âœ… role ì—†ì´ë„ ë™ì‚¬ë©´ ì ì„  ì—°ê²° ëŒ€ìƒ!
        if token.get("pos") not in {"AUX", "VERB"}:
            continue

        dep = token.get("dep", "").lower()
        if dep not in {"root", "conj", "xcomp", "ccomp"}:
            continue

        level = token.get("level")
        if level is None:
            continue

        idx1 = token["idx"]
        idx2 = None

        for t in parsed:
            if (
                t["idx"] > idx1 and
                t.get("level") == level and
                t.get("pos") in {"VERB", "AUX"} and
                t.get("dep", "").lower() in {"root", "conj", "xcomp", "ccomp"}
            ):
                has_subject_between = any(
                    s.get("role1") == "subject" and
                    s.get("level") == level and
                    idx1 < s["idx"] < t["idx"]
                    for s in parsed
                )
                if has_subject_between:
                    break
                idx2 = t["idx"]

        if idx2 and (idx1, idx2) not in visited:
            visited.add((idx1, idx2))
            line = symbols_by_level.setdefault(level, [" " for _ in range(line_length)])
            for i in range(idx1 + 1, idx2):
                if line[i] == " ":
                    line[i] = "."


# â— memory["symbols"] ë‚´ìš©ì„ ì¶œë ¥í•˜ê¸° ìœ„í•´ ë§Œë“  í•¨ìˆ˜
def symbols_to_diagram(sentence: str):
    output_lines = []

    line_length = memory["sentence_length"]
    parsed = memory.get("parsed")

    # âœ… ìƒˆ ë°©ì‹ìœ¼ë¡œ ì‹œì œ/ìƒ/íƒœ symbol map ì¶œë ¥
    tav_line = [" " for _ in range(line_length)]
    symbol_map = memory.get("verb_attribute", {}).get("symbol_map", {})
    for idx, symbol in symbol_map.items():
        if 0 <= idx < line_length:
            tav_line[idx] = symbol
    output_lines.append("".join(tav_line))  # â† ì²« ì¤„ë¡œ ì¶œë ¥

    # âœ… ë¬¸ì¥ í…ìŠ¤íŠ¸ ì¤„
    output_lines.append(sentence)

    # âœ… bridge(âˆ©) ë° â—‹â–¡ ì‹¬ë³¼ ì¶œë ¥
    if parsed:
        apply_aux_to_mverb_bridge_symbols_each_levels(parsed, sentence)

#   clean_empty_symbol_lines()

    for level in sorted(memory["symbols_by_level"]):
        output_lines.append(''.join(memory["symbols_by_level"][level]))

    draw_dot_bridge_across_verb_group(parsed)

    return '\n'.join(output_lines)


def t(sentence: str):
    print(f"\nğŸ“˜ Sentence: {sentence}")

    # âœ… ë©”ëª¨ë¦¬ ë¨¼ì € ì´ˆê¸°í™” (ë¬¸ì¥ ê¸¸ì´ ê¸°ë°˜ ì„¤ì • í¬í•¨)
    init_memorys(sentence)

    # âœ… spaCy íŒŒì‹± + ì—­í•  ë¶„ì„
    parsed = spacy_parsing_backgpt(sentence)
    memory["parsed"] = parsed

    chunk_info_list = assign_chunk_role2(parsed)
    NounChunk_combine_apply_to_upverb(parsed)
    apply_symbols(parsed)
    apply_chunk_function_symbol(parsed)
    apply_chunk_symbols_overwrite(chunk_info_list)
    draw_dot_bridge_across_verb_group(parsed)

    # âœ… morph ìƒì„¸ ì¶œë ¥
    print("\nğŸ“Š Full Token Info with Annotations:")
    doc = nlp(sentence)
    for token in doc:
        morph = token.morph.to_dict()
        idx = token.idx
        text = token.text
        role1 = next((t.get("role1") for t in parsed if t["idx"] == idx), None)
        role2 = next((t.get("role2") for t in parsed if t["idx"] == idx), None)
        combine = next((t.get("combine") for t in parsed if t["idx"] == idx), None)
        level = next((t.get("level") for t in parsed if t["idx"] == idx), None)

        combine_str = (
            "[" + ", ".join(
                f"{c['text']}({c['idx']}):" + (
                    f"{c['role1']}" if 'role1' in c else ""
                ) + (
                    f"/{c['role2']}" if 'role2' in c else ""
                )
                for c in combine
            ) + "]"
            if combine else "None"
        )

        child_texts = [child.text for child in token.children]

        print(f"â— idx({idx}), text({text}), role1({role1}), role2({role2}), combine({combine_str})")
        print(f"  level({level}), POS({token.pos_}), DEP({token.dep_}), TAG({token.tag_}), HEAD({token.head.text})")
        print(f"  lemma({token.lemma_}), is_stop({token.is_stop}), is_punct({token.is_punct}), is_title({token.is_title})")
        print(f"  morph({morph})")
        print(f"  children({child_texts})")
        print("")
        
    # âœ… ë„ì‹ ì¶œë ¥
    print("ğŸ›  Diagram:")
    print(symbols_to_diagram(sentence))


# ë¬¶ìŒ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def t1(sentence: str):
    # âœ… ë©”ëª¨ë¦¬ ë¨¼ì € ì´ˆê¸°í™” (ë¬¸ì¥ ê¸¸ì´ ê¸°ë°˜ ì„¤ì • í¬í•¨)
    init_memorys(sentence)

    # âœ… spaCy íŒŒì‹± + ì—­í•  ë¶„ì„
    parsed = spacy_parsing_backgpt(sentence)
    memory["parsed"] = parsed
    # âœ… ë„ì‹í™” ë° ì¶œë ¥
    chunk_info_list = assign_chunk_role2(parsed)
    NounChunk_combine_apply_to_upverb(parsed)
    apply_chunk_function_symbol(parsed)
    apply_symbols(parsed)
    apply_chunk_symbols_overwrite(chunk_info_list)
    draw_dot_bridge_across_verb_group(parsed)
    print("ğŸ›  Diagram:")
    print(symbols_to_diagram(sentence))



# â— ëª¨ë“ˆ ì™¸ë¶€ ì‚¬ìš©ì„ ìœ„í•œ export
__all__ = [
    "rule_based_parse",
    "guess_role",
    "assign_noun_complement_for_SVOC_noun_only",
    "repair_object_from_complement",
    "guess_combine",
    "assign_level_triggers",
    "assign_level_ranges",
    "spacy_parsing_backgpt",
    "gpt_parsing_withprompt",
    "init_memorys",
    "apply_symbols",
    "symbols_to_diagram",
    "t", "t1"
]

# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ìë™ ì‹¤í–‰


# â— ë¶„ì„ API ì—”ë“œí¬ì¸íŠ¸
@app.post("/analyze", response_model=AnalyzeResponse)  # sentenceë¥¼ ë°›ì•„ "sentence"ì™€ "diagramming" ë¦¬í„´
async def analyze(request: AnalyzeRequest):            # sentenceë¥¼ ë°›ì•„ ë‹¤ìŒ ì²˜ë¦¬ë¡œ ë„˜ê¹€
    init_memorys(request.sentence)                     # ì´ í•¨ìˆ˜ë¡œ ë©”ëª¨ë¦¬ ë‚´ìš© ì±„ì›€ ë˜ëŠ” ì´ˆê¸°í™”
    parsed = spacy_parsing_backgpt(request.sentence)               # GPTì˜ íŒŒì‹±ê²°ê³¼ë¥¼ parsedì— ì €ì¥
    memory["parsed"] = parsed
    chunk_info_list = assign_chunk_role2(parsed)
    NounChunk_combine_apply_to_upverb(parsed)
    apply_chunk_function_symbol(parsed)
    apply_symbols(parsed)                              # parsed ê²°ê³¼ì— ë”°ë¼ ì‹¬ë³¼ë“¤ì„ ë©”ëª¨ë¦¬ì— ì €ì¥ì¥
    apply_chunk_symbols_overwrite(chunk_info_list)
    draw_dot_bridge_across_verb_group(parsed)
    return {"sentence": request.sentence,
            "diagramming": symbols_to_diagram(request.sentence),
            "verb_attribute": memory.get("verb_attribute", {})
    }


# â— spaCy íŒŒì‹± ê´€ë ¨
@app.post("/parse")
def parse_text(req: ParseRequest):
    doc = nlp(req.text)
    result = []

    for token in doc:
        morph = token.morph.to_dict()
        result.append({
            "idx": token.idx, "text": token.text, "pos": token.pos_, "tag": token.tag_,
            "dep": token.dep_, "head": token.head.text, "head_idx": token.head.idx,
            "tense": morph.get("Tense"), "aspect": morph.get("Aspect"), "form": morph.get("VerbForm"),
            "voice": morph.get("Voice"), "morph": morph, "lemma": token.lemma_,
            "is_stop": token.is_stop, "is_punct": token.is_punct, "is_alpha": token.is_alpha,
            "ent_type": token.ent_type_, "is_title": token.is_title,
            "children": [child.text for child in token.children]
        })

    return {"result": result}

# â— ì»¤ìŠ¤í…€ OpenAPI JSON ì œê³µ ì—”ë“œí¬ì¸íŠ¸
# FastAPIì—ì„œ custom-openapi.json ì—”ë“œí¬ì¸íŠ¸ë¥¼ ë§Œë“¤ì–´ì„œ GPTsì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•¨.
@app.get("/custom-openapi.json", include_in_schema=False)
async def serve_openapi():
    file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "openapi.json"))
    # file_path = os.path.join(os.path.dirname(__file__), "..", "openapi.json")
    return FileResponse(file_path, media_type="application/json")

# â— ì•„ë˜ ì—”ë“œí¬ì¸íŠ¸ëŠ” GET /ping ìš”ì²­ì— ëŒ€í•´ {"message": "pong"} ì‘ë‹µì„ ì¤€ë‹¤.
@app.get("/ping")
async def ping():
    return JSONResponse(content={"message": "pong"}, status_code=200)
##
