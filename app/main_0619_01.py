import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import os, json, re
import spacy
import uvicorn
from fastapi import FastAPI
from fastapi.responses import JSONResponse, FileResponse  # renderì— 10ë¶„ ë‹¨ìœ„ Ping ë³´ë‚´ê¸°ë¥¼ ìœ„í•´ ì¶”ê°€
from pydantic import BaseModel
# ì•„ë˜ api_key= ê¹Œì§€ëŠ” .env íŒŒì¼ì—ì„œ OpenAIí‚¤ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° ê´€ë ¨ ë¶€ë¶„ 
from openai import OpenAI
from dotenv import load_dotenv

# â— í™˜ê²½ ì„¤ì •
load_dotenv()

api_key = os.getenv("API_KEY") or os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("âŒ OPENAI_API_KEY is not set in environment variables.")
client = OpenAI(api_key=api_key)

app = FastAPI()  # FastAPI() ê°ì²´ë¥¼ ìƒì„±í•´ì„œ ì´í›„ ë¼ìš°íŒ…ì— ì‚¬ìš©

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸°, ì—†ìœ¼ë©´ 'en_core_web_sm' ê¸°ë³¸ê°’
model_name = os.getenv("SPACY_MODEL", "en_core_web_trf")

try:
    nlp = spacy.load(model_name)
except OSError:
    # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ í›„ ë‹¤ì‹œ ë¡œë“œ
    from spacy.cli import download
    download(model_name)
    nlp = spacy.load(model_name)

# â— ì‹¬ë³¼ ë§¤í•‘
role_to_symbol = {
    "verb": "â—¯",
    "object": "â–¡",
    "indirect object": "â–¡",
    "direct object": "â–¡",
    "prepositional object": "â–¡",
    "preposition": "â–½",
    "conjunction": "â—‡",
    "noun subject complement": "[",
    "adjective subject complement": "(",
    "noun object complement": "[",
    "adjective object complement": "("
}

verb_attr_symbol = {
    "present tense": "|",
    "past tense": ">",
    "perfect aspect": "P",
    "progressive aspect": "i",
    "passive voice": "^",
    "subjunctive mood": "ã€‹"
}

#verbals_symbol = {
#    "bare infinitive": "R",
#    "to infinitive": "to.R",
#    "gerund": "R.ing",
#    "present participle": "R.ing",
#    "past participele": "R.ed"
#}

relative_words_symbol = {
    "relative pronoun": "[X]",
    "relative adjective": "(X)",
    "relative adverb": "<X>",
    "compound relative pronoun": "[e]",
    "compound relative adjective": "(e)",
    "compound relative adverb": "<e>",
    "interrogative pronoun": "[?]",
    "interrogative adjective": "(?)",
    "interrogative adverb": "<?>"
}

# ì „ì²´ ì‹¬ë³¼ í†µí•© ë”•ì…”ë„ˆë¦¬
symbols_all = {
    "role": role_to_symbol,
    "verb_attr": verb_attr_symbol,
#    "verbals": verbals_symbol,
    "relatives": relative_words_symbol
}

# â— ë©”ëª¨ë¦¬ êµ¬ì¡° / ë©”ëª¨ë¦¬ ì´ˆê¸°í™”í™”
memory = {
    "symbols_by_level": {},
    "symbols_all": symbols_all
}


# level ë°œìƒ íŠ¸ë¦¬ê±° dep ëª©ë¡ (ì „ì—­ìœ¼ë¡œ í†µì¼)
level_trigger_deps = [
    "relcl", "acl", "advcl", "advmodcl", "ccomp", "xcomp", "csubj", "parataxis"
]

# guess_role() í•¨ìˆ˜ì—ì„œëŠ” ì‚¬ìš©ê¸ˆì§€("csubj"ë¥¼ ì œì™¸ì‹œì¼œì•¼í•¨)
is_subject_deps = [ "nsubj", "nsubjpass", "csubj"]

all_nouchunk_types = {
    "subclause_noun", "to.R_noun", "R.ing_ger_noun"
}

# ğŸ“˜ ë³´ì–´ê°€ **ëª…ì‚¬ë§Œ** ê°€ëŠ¥í•œ SVOC ë™ì‚¬
SVOC_noun_only = {
    "name", "appoint", "elect", "dub", "label", "christen", "nominate"
}

# ğŸ“™ ë³´ì–´ê°€ **í˜•ìš©ì‚¬ë§Œ** ê°€ëŠ¥í•œ SVOC ë™ì‚¬
SVOC_adj_only = {
    "find", "keep", "leave", "consider"  # ì¼ë¶€ ë³´ì–´ ëª…ì‚¬ë„ ê°€ëŠ¥í•˜ê¸´ í•˜ì§€ë§Œ ê±°ì˜ í˜•ìš©ì‚¬ ìš°ìœ„
}

# ğŸ“— ë³´ì–´ê°€ **ëª…ì‚¬/í˜•ìš©ì‚¬ ë‘˜ ë‹¤** ê°€ëŠ¥í•œ SVOC ë™ì‚¬
SVOC_both = {
    "make", "call", "consider", "declare", "paint", "think", "judge"
}

noSubjectComplementVerbs = {
    "live", "arrive", "go", "come", "sleep", "die", "run", "walk", "travel", "exist", "happen"
}

noObjectVerbs = {
    "die", "arrive", "exist", "go", "come", "vanish", "fall", "sleep", "occur"
}

modalVerbs_present = {"will", "shall", "can", "may", "must"}
modalVerbs_past = {"would", "should", "could", "might"}
modalVerbs_all = modalVerbs_present | modalVerbs_past

beVerbs = {"be", "am", "are", "is", "was", "were", "been", "being"}

notbeLinkingVerbs_onlySVC = {
    "become", "come", "go", "fall", "sound", "look", "smell", "taste", "seem"
}
notbeLinkingVerbs_SVCSVO = {
    "get", "turn", "grow", "feel"
}
netbeLinkingVerbs_all = notbeLinkingVerbs_SVCSVO | notbeLinkingVerbs_onlySVC

dativeVerbs = {
    "give", "send", "offer", "show", "lend", "teach", "tell", "write", "read", "promise",
    "sell", "pay", "pass", "bring", "buy", "ask", "award", "grant", "feed", "hand", "leave", "save", 
    "bring", "bake", "build", "cook", "sing", "make"  # dative verbë¡œ ì‚¬ìš© ë“œë¬¸ ê²ƒë“¤
}


# spaCyê°€ ì „ì¹˜ì‚¬ë¡œ ì˜¤ì¸ íƒœê¹…í•˜ëŠ” íŠ¹ìˆ˜ ë‹¨ì–´ë“¤
blacklist_preposition_words = {"due", "according"}


# â— ìš”ì²­/ì‘ë‹µ ëª©ë¡
class AnalyzeRequest(BaseModel):   # ì‚¬ìš©ìê°€ ë³´ë‚¼ ìš”ì²­(sentence) ì •ì˜
    sentence: str

class AnalyzeResponse(BaseModel):  # ì‘ë‹µìœ¼ë¡œ ëŒë ¤ì¤„ ë°ì´í„°(sentence, diagramming) ì •ì˜
    sentence: str
    diagramming: str               # "     â—‹______â–¡__[         "
    verb_attribute: dict

class ParseRequest(BaseModel):     # spaCy ê´€ë ¨ ì„¤ì •
    text: str


# rule ê¸°ë°˜ ë¶„ì„ ë¼ˆëŒ€ í•¨ìˆ˜ ì„ ì–¸
def rule_based_parse(tokens):
    result = []
    for t in tokens:
        t["children"] = [c["idx"] for c in tokens if c["head_idx"] == t["idx"]]
        t["role1"] = None
        t["role2"] = None
        t["role3"] = None


        # role ì¶”ë¡ 
        role1 = guess_role(t, tokens)
        if role1:
            t["role1"] = role1  # combineì—ì„œ ì“°ì¼ ìˆ˜ ìˆìŒ
            t["role2"] = role1  
            t["role3"] = role1  


    # 'name'ê³¼ ê°™ì€ ë™ì‚¬ê°€ ìˆëŠ” SVOCêµ¬ì¡°ì—ì„œ ëª©ì ë³´ì–´ë¥¼ ì˜ëª» íƒœê¹…í•˜ëŠ” ê²ƒ ë³´ì • í•¨ìˆ˜
    result = tokens  # ê¸°ì¡´ tokensì„ ìˆ˜ì •í•˜ë©° ê³„ì† ì‚¬ìš©
    result = assign_noun_complement_for_SVOC_noun_only(result)

    # âœ… ë³´ì–´ ê¸°ë°˜ object ë³µêµ¬ ìë™ ì ìš©
    result = repair_object_from_complement(result)        


######################################## ì‹ ê²½ì„ ì¨ì•¼í•  íŠ¹ë³„ì˜ˆì™¸ì²˜ë¦¬ ë¶€ë¶„ ###################################

    ## íŠ¹ë³„ì˜ˆì™¸ : ê³„ì¸µë°œìƒ ccompì˜ ìì‹ì´ toë¶€ì •ì‚¬ì´ê³ , toë¶€ì •ì‚¬ì˜ ì£¼ì²´ì¸ ì•ë‹¨ì–´ë¥¼ nsubjë¡œ íƒœê¹…í•˜ëŠ”ë°,
    #            nsubjê°€ ë©ì–´ë¦¬ìš”ì†Œ ì‹œì‘ë‹¨ì–´ê°€ ë˜ë²„ë¦¬ëŠ” ê²½ìš° nsubj(you)ë¥¼ objectë¡œ ì…ë ¥,
    #            toë¥¼ noun object complementë¡œ ì…ë ¥ (ì˜ˆë¬¸ : I want you to succeed.)

    for t in tokens:
        if t.get("dep") == "ccomp":
            #ì´ê²½ìš° ccompì˜ ìì‹ì€ toì•ë‹¨ì–´(nsubj), to(TO) ëª¨ë‘ ccompë¥¼ headë¡œ ë³¸ë‹¤.
            children = [child for child in tokens if child.get("head_idx") == t["idx"]]
            nsubj_child = next((child for child in children if child.get("dep") == "nsubj"), None)
            to_child = next((child for child in children if child.get("tag") == "TO"), None)

            if nsubj_child and to_child:
                print(f"[DEBUG] ccomp '{t['text']}' has nsubj '{nsubj_child['text']}' + to '{to_child['text']}'")
                nsubj_child["role1"] = "object"
                to_child["role1"] = "noun object complement"
    # assign_level_trigger_rangesì—ì„œëŠ” youì™€ toì˜ ë ˆë²¨ê°’ì„ ë³´ì •í•¨.

#######################################################################################################

    return result



# role ì¶”ë¡  í•¨ìˆ˜
def guess_role(t, all_tokens=None):  # all_tokens ì¶”ê°€ í•„ìš”
    dep = t.get("dep")
    pos = t["pos"]
    head_idx = t.get("head_idx")

    # âœ… Subject
    if dep in {"nsubj", "nsubjpass"}:
        return "subject"

    # âœ… Main Verb: beë™ì‚¬ í¬í•¨, ì¢…ì†ì ˆë„ ê³ ë ¤
    if pos in ["VERB", "AUX"] and (dep in level_trigger_deps or dep == "root"):
        return "verb"

    # âœ… ë“±ìœ„ì ‘ì†ì‚¬ ë‹¤ìŒ ë³‘ë ¬ ë™ì‚¬ (conj)ë„ verb role ë¶€ì—¬
    if pos == "VERB" and dep == "conj":
        if any(t["idx"] == head_idx and t.get("role1") == "verb" for t in all_tokens):
            return "verb"


    # âœ… Indirect Object
    if dep in ["iobj", "dative"]:
        return "indirect object"

    # âœ… Direct Object (SVOO êµ¬ì¡° íŒë‹¨)
    if dep in ["dobj", "obj"]:
        if head_lemma := next((t["lemma"] for t in all_tokens if t["idx"] == head_idx), None):
            if head_lemma in noObjectVerbs:
                return None  # âŒ ëª©ì ì–´ ê¸ˆì§€ ë™ì‚¬ â†’ ë¬´ì‹œ

        # âœ… ê¸°ì¡´ object íŒë‹¨ ë¡œì§
        if all_tokens:
            for other in all_tokens:
                if other.get("dep") in ["iobj", "dative"] and other.get("head_idx") == head_idx:
                    return "direct object"
        return "object"


    # âœ… Preposition: ê¸°ë³¸ prep, agent + ë³´ì™„ (pcomp), ë‹¨ blacklist ë‹¨ì–´ëŠ” ì œì™¸
    if (
        (dep in ["prep", "agent"] or (dep == "pcomp" and pos == "ADP" and t.get("tag") == "IN"))
        and t["text"].lower() not in blacklist_preposition_words
    ):
        return "preposition"

    # âœ… Prepositional Object (by ~pobj êµ¬ì¡° ì»¤ë²„)
    # ìœ„ Preposition Roleê²°ì •ì‹œ blacklist ë‹¨ì–´ì— ì˜í•´ ì „ì¹˜ì‚¬ì˜ ëª©ì ì–´ë¥¼ ëª»ì°¾ëŠ” ë¬¸ì œ ë³´ì •
    if dep == "pobj":
        head_token = next((t for t in all_tokens if t["idx"] == head_idx), None)
        if head_token and (
            head_token.get("role1") == "preposition"
            or (
                head_token["text"].lower() in blacklist_preposition_words and
                (
                    head_token.get("pos") == "ADP" or
                    head_token.get("dep") == "prep" or
                    head_token.get("tag") == "IN"
                )
            )
        ):
            return "prepositional object"
    
    # âœ… Conjunction or Clause Marker (ì ‘ì†ì‚¬)
    if dep in ["cc", "mark"]:
        return "conjunction"

    # âœ… Subject Complement (SVC êµ¬ì¡°)
    if dep in ["attr", "acomp"]:
        if head_lemma := next((t["lemma"] for t in all_tokens if t["idx"] == head_idx), None):
            if head_lemma in noSubjectComplementVerbs:
                return None  # âŒ ë³´ì–´ ë¶ˆê°€ ë™ì‚¬ â†’ ì°¨ë‹¨

        if pos in ["NOUN", "PROPN", "PRON"]:
            return "noun subject complement"
        elif pos == "ADJ":
            return "adjective subject complement"

    # âœ… Object Complement (ì •ìƒ ì¼€ì´ìŠ¤: dep=oprd, xcomp)
    if dep in ["oprd", "xcomp", "ccomp"]:
        if pos in ["NOUN", "PROPN", "PRON"]:
            return "noun object complement"
        elif pos == "ADJ":
            return "adjective object complement"

    # âœ… Object Complement (ë³´ì™„ ì¼€ì´ìŠ¤: dep=advmod, pos=ADJ, head=VERB, dobj ì¡´ì¬ ì‹œ)
    if dep == "advmod" and pos == "ADJ":
        for tok in all_tokens:
            if tok["idx"] == head_idx and tok["pos"] == "VERB":
                obj_exists = any(
                    c.get("head_idx") == tok["idx"] and c.get("dep") in ["dobj", "obj"]
                    for c in all_tokens
                )
                if obj_exists:
                    return "adjective object complement"
                
    # âœ… ê·¸ ì™¸ëŠ” DrawEnglish ë„ì‹ì—ì„œ ì‚¬ìš© ì•ˆ í•¨
    return None


# depê°€ dativeì—¬ì„œ indiret objectê°€ ìˆëŠ”ë°, ë’¤ìª½ì— direct object roleì´ ì—†ëŠ” ê²½ìš° ë³´ì •
def recover_direct_object_from_indirect(parsed):
    """
    SVOO ë¬¸ì¥ì—ì„œ indirect objectì— ëŒ€í•´ appos êµ¬ì¡°ì˜ direct objectë¥¼ ë³µì›
    """
    for token in parsed:
        if token.get("role1") == "indirect object":
            token_idx = token.get("idx")

            for child in parsed:
                if (
                    child.get("head_idx") == token_idx and
                    child.get("dep") == "appos" and
                    child.get("pos") in {"NOUN", "PROPN"}
                ):
                    child["role1"] = "direct object"
                    break

    return parsed


# ëª©ì ë³´ì–´ë¡œ ëª…ì‚¬ë§Œ ì·¨í•˜ëŠ” ë™ì‚¬ ì‚¬ìš© ë¬¸ì¥ì—ì„œ ëª©ì ë³´ì–´ë¥¼ ì˜ëª» íƒœê¹…í•˜ëŠ” ê²ƒ ë³´ì •
# ê·¸ í›„ ì•„ë˜ repair_object_from_complement()í•¨ìˆ˜ë¥¼ í†µí•´ ëª©ì ì–´ë¥¼ ë³´ì •í•¨
def assign_noun_complement_for_SVOC_noun_only(parsed):
    """
    SVOC êµ¬ì¡° ë™ì‚¬ë“¤(SVOC_noun_only ì‚¬ì „ì— ë“±ë¡)ì˜ ëª©ì ë³´ì–´ê°€ spaCyì—ì„œ ì˜ëª» íƒœê¹…ëœ ê²½ìš°
    noun object complementë¡œ 1íšŒ ë³´ì • ë‹¨, object ì´í›„ì˜ ë‹¨ì–´ë§Œ ëŒ€ìƒìœ¼ë¡œ í•œë‹¤.
    """
    applied = False

    for i, token in enumerate(parsed):
        if token.get("lemma") in SVOC_noun_only and token.get("pos") in ["VERB", "AUX"]:
            verb_idx = token["idx"]

            # object í™•ì¸
            obj = next((t for t in parsed if t.get("head_idx") == verb_idx and t.get("dep") in ["dobj", "obj"]), None)
            if not obj:
                continue

            obj_idx = obj["idx"]

            # ë³´ì–´ í›„ë³´ ì°¾ê¸° : objedt ë’¤ì— ìˆëŠ” ëª…ì‚¬ì‚¬
            for t in parsed:
                if applied:
                    break

                if (
                    t.get("idx") > obj_idx and  # âœ… object ì´í›„ì— ë“±ì¥í•œ ë‹¨ì–´ë§Œ
                    t.get("head_idx") == verb_idx and
                    t.get("dep") in ["nsubj", "nmod", "attr", "appos", "npadvmod", "ccomp"] and
                    t.get("pos") in ["NOUN", "PROPN"]
                ):
                    t["role1"] = "noun object complement"
                    applied = True
                    break

    return parsed

# ëª©ì ë³´ì–´ë¡œ í˜•ìš©ì‚¬ë§Œ ë˜ëŠ” í˜•ìš©ì‚¬/ëª…ì‚¬ë¥¼ ëª¨ë‘ ì·¨í•˜ëŠ” ë™ì‚¬ ì‚¬ìš© ë¬¸ì¥ì—ì„œ ëª©ì ë³´ì–´ë¥¼ ì˜ëª» íƒœê¹…í•˜ëŠ” ê²ƒ ë³´ì •
# ê·¸ í›„ ì•„ë˜ repair_object_from_complement()í•¨ìˆ˜ë¥¼ í†µí•´ ëª©ì ì–´ë¥¼ ë³´ì •í•¨
# ì˜ˆ: "She painted the wall green."
def assign_adj_object_complement_when_compound_object(parsed):
    for verb in parsed:
        if verb.get("pos") != "VERB":
            continue

        verb_lemma = verb.get("lemma")
        if verb_lemma not in SVOC_adj_only and verb_lemma not in SVOC_both:
            continue

        verb_idx = verb["idx"]

        # ë³´ì–´ í›„ë³´: VERBì˜ ìì‹ ì¤‘ dep=obj, pos=ADJ
        for t in parsed:
            if (
                t.get("head_idx") == verb_idx and
                t.get("dep") in ["dobj", "obj"] and
                t.get("pos") == "ADJ"
            ):
                # âœ… ì´ ADJì˜ childrenì— compoundê°€ ë¶™ì–´ ìˆìœ¼ë©´ ë³´ì • ëŒ€ìƒ
                children = [c for c in parsed if c.get("head_idx") == t["idx"]]
                has_compound = any(
                    c.get("dep") == "compound" and c.get("pos") == "NOUN"
                    for c in children
                )
                if has_compound:
                    t["role1"] = "adjective object complement"

    return parsed

# SVOC_both ë™ì‚¬ì— ì†í•´ ìˆê³ , ë’¤ì— role(object)ê°€ ìˆê³ , ê·¸ ë’¤ì— advcl, ADJ ì´ë©´ì„œ HEADê°€ objectì™€ ê°™ì„ë•Œ
# adjective object complementë¡œ ë³´ì •.  ì˜ˆ: He painted the kitchen walls blue.
def assign_adj_complement_for_advcl_adjective(parsed):
    """
    spaCyê°€ í˜•ìš©ì‚¬ ëª©ì ë³´ì–´ë¥¼ advclë¡œ ì˜ëª» íƒœê¹…í–ˆì„ ë•Œ ë³´ì •
    ì˜ˆ: "He painted the walls blue."
    """
    for verb in parsed:
        if verb.get("pos") != "VERB":
            continue
        if verb.get("lemma") not in SVOC_both:
            continue

        verb_idx = verb["idx"]

        # 1. object ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
        obj = next(
            (t for t in parsed if t.get("head_idx") == verb_idx and t.get("role1") in ["object", "direct object"]),
            None
        )
        if not obj:
            continue

        obj_idx = obj["idx"]

        # 2. object ì´í›„ ë“±ì¥í•œ í˜•ìš©ì‚¬ ì¤‘ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ ë³´ì–´ë¡œ ê°„ì£¼
        for t in parsed:
            if (
                t.get("idx") > obj_idx and
                t.get("head_idx") == verb_idx and
                t.get("dep") == "advcl" and
                t.get("pos") == "ADJ"
            ):
                t["role1"] = "adjective object complement"
    return parsed


# ëª©ì ë³´ì–´(object complement)ê°€ ìˆëŠ”ë°, ì•ìª½ ëª©ì ì–´ë¥¼ nsubj(subject)ë¡œ ì˜ëª» íƒœê¹…í•˜ëŠ” ê²½ìš° ì˜ˆì™¸ì²˜ë¦¬
def repair_object_from_complement(parsed):
    for item in parsed:
        if item.get("role1") in ["noun object complement", "adjective object complement"]:
            complement_children = item.get("children", [])

            # 1ï¸âƒ£ ë¨¼ì €: nsubj ë¨¼ì € ì°¾ê¸°
            found_object = False
            for t in parsed:
                if t.get("dep") == "nsubj" and t.get("idx") in complement_children:
                    t["role1"] = "object"
                    found_object = True
                    break  # âœ… ë‹¨ 1íšŒë§Œ ë³´ì •

            # 2ï¸âƒ£ ê·¸ ë‹¤ìŒ: compound ì°¾ê¸° (nsubj ì—†ì„ ê²½ìš°ë§Œ)
            if not found_object:
                compound_candidates = [
                    t for t in parsed 
                    if (
                        t["idx"] in complement_children and
                        t.get("dep") == "compound" and
                        t.get("pos") == "NOUN" and
                        t.get("head_idx") == item["idx"]
                    )
                ]

                if compound_candidates:
                    compound_candidates.sort(key=lambda x: x["idx"])
                    compound_candidates[0]["role1"] = "object"

    return parsed



# combine ì¶”ë¡  í•¨ìˆ˜
def guess_combine(token, all_tokens):
    token_role1 = token.get("role1")
    token_idx = token.get("idx")
    combine = []

    token_current_level = token.get("level")
    token_head_idx = token.get("head_idx")

    # âœ… Verb â†’ object / complement (SVO, SVC)
    if token_role1 == "verb":
        for t in all_tokens:
            if t.get("idx", -1) <= token_idx: # ì´ì „ í† í°ì´ë©´ continue(ë‹¤ìŒ í† í°ë¶€í„° ì°¾ìŒ)
                continue
            t_head_idx = t.get("head_idx")
            t_head_token = next((x for x in all_tokens if x.get("idx") == t_head_idx), None)
            t_head2_idx = t_head_token.get("head_idx") if t_head_token else None
            t_level = t.get("level")

            if (
                t_head_idx == token_idx or t_head2_idx == token_idx
                and t["idx"] > token_idx  # ğŸ”§ ì˜¤ë¥¸ìª½ ë°©í–¥ ì—°ê²°ë§Œ í—ˆìš©
                and int(t_level) == token_current_level
            ):
                r = t.get("role1")
                if r in [
                    "object",
                    "indirect object",
                    "direct object",
                    "noun subject complement",
                    "adjective subject complement",
#                    "noun object complement",
#                    "adjective object complement"  # ğŸ”§ ë³´ì–´ë„ ì—°ê²°ë˜ê²Œ!
                ]:
                    combine.append({"text": t["text"], "role1": r, "idx": t["idx"]})

                    # âœ… ë³´ì™„: indirect objectê°€ ìì‹ ê°–ê³  ìˆìœ¼ë©´ ê·¸ ì¤‘ direct objectë„ ì—°ê²°
                    # â™¥â™¥â™¥ ì´ ë³´ì™„í•¨ìˆ˜ë¥¼ ì ìš©í•´ì•¼ í•˜ëŠ” ë¬¸ì¥ì„ ëª»ì°¾ê² ìŒ..
                    if r == "indirect object":
                        children = [c for c in all_tokens if c.get("head_idx") == t["idx"]]
                        for c in children:
                            if (
                                c.get("role1") in ["direct object", "object"]
                                and c["idx"] > t["idx"]  # ğŸ”§ í•µì‹¬ ì¶”ê°€
                            ):
                                combine.append({"text": c["text"], "role1": c["role1"], "idx": c["idx"]})

                    break

    # âœ… Indirect object / object â†’ direct object (SVOO êµ¬ì¡°)
    if token_role1 in ("indirect object", "object"):

        for t in all_tokens:
            if t.get("idx", -1) <= token_idx:
                continue
            t_head_idx = t.get("head_idx")
            t_head_token = next((x for x in all_tokens if x.get("idx") == t_head_idx), None)
            t_head2_idx = t_head_token.get("head_idx") if t_head_token else None
            t_level = t.get("level")

            if (
                t.get("role1") == "direct object" and
                t.get("idx") > token_idx and
                int(t_level) == token_current_level
            ):

                if t_head_idx == token_head_idx or t_head2_idx == token_head_idx:
                    combine.append({
                        "text": t["text"], "role1": "direct object", "idx": t["idx"]
                    })

    # âœ… Object â†’ object complement (SVOC êµ¬ì¡°)
    if token_role1 == "object":
        for t in all_tokens:
            t_role1 = t.get("role1") or ""
            t_level = t.get("level")
            if t_role1 in ("noun object complement", "adjective object complement"):
                t_head = t.get("head_idx")
                token_head = token.get("head_idx")
                if (
                    token_idx in [c.get("idx") for c in all_tokens if c.get("head_idx") == t.get("idx")]
                    or (t_head is not None and t_head == token_head)
                ):
                    combine.append({"text": t["text"], "role1": t["role1"], "idx": t["idx"]})
                    continue

                # ğŸ”¹ ì¶”ê°€ ì—°ê²° ì¡°ê±´: ë³´ì–´ê°€ objectë³´ë‹¤ ë’¤ì— ìˆê³ , headëŠ” ë™ì¼í•œ ë™ì‚¬
                if (
                    t["idx"] > token_idx and
                    t.get("dep") in {"advcl", "oprd", "xcomp", "ccomp"} and
                    t.get("pos") == "ADJ" and
                    t.get("head_idx") == token.get("head_idx") and
                    int(t_level) == token_current_level
                ):
                    combine.append({"text": t["text"], "role1": t["role1"], "idx": t["idx"]})

    # âœ… Preposition â†’ prepositional object
    if token_role1 == "preposition":
        for t in all_tokens:
            t_level = t.get("level")
            if (
                t.get("role1") == "prepositional object" and t.get("head_idx") == token_idx
                and int(t_level) == token_current_level
            ):
                print(f"[DEBUG] prepositional object t.level={t.get('level')}, token.level={token_current_level}")
                combine.append({"text": t["text"], "role1": "prepositional object", "idx": t["idx"]})

        # 2ï¸âƒ£ ì˜ˆì™¸ ë³´ì •: headê°€ due/accordingì¸ë°, ì´ tokenì´ ê·¸ ë’¤ì˜ "to"ì¼ ê²½ìš°
    for t in all_tokens:
        if (
            t.get("role1") == "prepositional object" and
            t.get("head_idx") in [
                b["idx"] for b in all_tokens if b["text"].lower() in blacklist_preposition_words
            ]
        ):
            # ğŸ‘‰ head token
            t_head_idx = t.get("head_idx")
            t_head_token = next((tok for tok in all_tokens if tok["idx"] == t_head_idx), None)

            # âœ… head_token ë‹¤ìŒì—ì„œ "to" ì°¾ê¸°
            to_token = next(
                (
                    tok for tok in all_tokens
                    if tok["text"].lower() == "to" and tok["idx"] > t_head_token["idx"]
                ),
                None
            )

            # âœ… ì´ tokenì´ ê·¸ "to"ì¼ ê²½ìš°ë§Œ ì—°ê²°
            if to_token and to_token["idx"] == token_idx:
                combine.append({"text": t["text"], "role1": t["role1"], "idx": t["idx"]})

    # âœ… combine ìˆì„ ê²½ìš°ë§Œ ë°˜í™˜
    return combine if combine else None


def assign_level_trigger_ranges(parsed):
    """
    ì¢…ì†ì ˆì„ ë‹´ë‹¹í•˜ëŠ” dep (relcl, acl, advcl, ccomp, xcomp)ì— ë”°ë¼
    í•´ë‹¹ ì ˆ ë²”ìœ„ì— level ê°’ì„ ë¶€ì—¬í•œë‹¤.
    
    - relcl, advcl, ccomp, xcomp: í•´ë‹¹ í† í° + children â†’ ë²”ìœ„ ê³„ì‚°
    - acl: í•´ë‹¹ í† í°ë¶€í„° children í¬í•¨í•˜ì—¬ ë²”ìœ„ ê³„ì‚° (ìê¸°ìì‹ ì´ ì—°ê²°ì–´)
    
    ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ì— level=Noneì¸ í† í°ë“¤ì— ëŒ€í•´ level=0ì„ ë¶€ì—¬í•œë‹¤.
    """

    clause_units = []  # ì ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    current_level = 1  # ì‹œì‘ì€ 1ë¶€í„° (0ì€ ìµœìƒìœ„ ì ˆìš©)
    reset_after_root = False  # âœ… ROOT ì´í›„ ë ˆë²¨ ì´ˆê¸°í™” í”Œë˜ê·¸
    prev_clause_indices = set()  # ì´ì „ ì ˆ ì¸ë±ìŠ¤ ì €ì¥ìš©

    for token in parsed:
        dep = token.get("dep")

        if dep == "root":
            reset_after_root = True
            continue  # ROOT ìì²´ëŠ” level íŠ¸ë¦¬ê±° ì•„ë‹˜

        if reset_after_root:
            current_level = 1
            reset_after_root = False

        if dep not in level_trigger_deps:
            continue
        
        if not is_valid_clause_trigger(token):
            continue

        

        all_clause_indices = []  # ì ˆ ë‹¨ìœ„ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë“¤ì„ ëª¨ì•„ë‘ 
        token_idx = token["idx"]
        clause_tokens = [token]  # ì‹œì‘ì€ ìê¸° ìì‹  í¬í•¨

        # âœ… childrenë„ ì ˆ ë²”ìœ„ì— í¬í•¨
        children = [t for t in parsed if t["head_idx"] == token_idx]
        clause_tokens.extend(children)

        clause_tokens = [token] + children
        clause_indices = sorted([t["idx"] for t in clause_tokens])
        
        all_clause_indices.append(clause_indices)
    
        print(f"[DEBUG] {all_clause_indices}")

        #is_nested = any(
        #    prev["indices"]
        #    and prev["indices"][0] < clause_indices[0] and prev["indices"][-1] > clause_indices[-1]
        #    for prev in clause_units[:-1]  # ìê¸° ìì‹  ì œì™¸
        #)

        for prev_unit in clause_units:
            prev_indices = prev_unit["indices"]
            overlap = set(clause_indices) & set(prev_indices)
            if overlap:
                # ìš°ì„ ìˆœìœ„ íŒë‹¨: ëˆ„ê°€ ë¨¼ì € ì‹œì‘í–ˆëŠ”ì§€
                if clause_indices[0] < prev_indices[0]:
                    # í˜„ì¬ clauseê°€ ë¨¼ì €ë‹ˆê¹Œ, í˜„ì¬ clauseì—ì„œ ì¤‘ë³µ ì œê±°
                    clause_tokens = [t for t in clause_tokens if t["idx"] not in overlap]
                    clause_indices = sorted([t["idx"] for t in clause_tokens])
                else:
                    # ì´ì „ clauseì—ì„œ ì¤‘ë³µ ì œê±°
                    prev_unit["tokens"] = [t for t in prev_unit["tokens"] if t["idx"] not in overlap]
                    prev_unit["indices"] = sorted([t["idx"] for t in prev_unit["tokens"]])

        clause_indices = sorted([t["idx"] for t in clause_tokens])

        # âœ… ì ˆ ë²”ìœ„ ì‹œì‘ ~ ë ê³„ì‚°
        if not clause_tokens:
            continue  # í˜¹ì‹œ ë‹¤ ì§€ì›Œì¡Œìœ¼ë©´ skip
        start_idx = min(t["idx"] for t in clause_tokens)
        end_idx = max(t["idx"] for t in clause_tokens)

        clause_units.append({
            "indices": clause_indices,
            "tokens": clause_tokens,
            "connector": token,
        })

        print(f"[DEBUG] {all_clause_indices}")
        print(f"[DEBUG ì‹œì‘ ë] {start_idx} {end_idx}")

        clause_indices = sorted([t["idx"] for t in clause_tokens])
        clause_indices_set = set(clause_indices)

        # âœ… level ë¶€ì—¬
        for t in parsed:
            if start_idx <= t["idx"] <= end_idx:
                if (
                    t.get("level") is None
                    #or t["idx"] in prev_clause_indices  # ì´ì „ ì ˆê³¼ ê²¹ì¹˜ëŠ” ê²½ìš°ë§Œ ë®ì–´ì“°ê¸° í—ˆìš©
                    #or not is_nested
                ):
                    t["level"] = current_level
        
        prev_clause_indices = clause_indices_set

######################################## ì‹ ê²½ì„ ì¨ì•¼í•  íŠ¹ë³„ì˜ˆì™¸ì²˜ë¦¬ ë¶€ë¶„ ###################################

    ## íŠ¹ë³„ì˜ˆì™¸ : ê³„ì¸µë°œìƒ ccompì˜ ìì‹ì´ toë¶€ì •ì‚¬ì´ê³ , toë¶€ì •ì‚¬ì˜ ì£¼ì²´ì¸ ì•ë‹¨ì–´ë¥¼ nsubjë¡œ íƒœê¹…í•˜ëŠ”ë°,
    #            nsubjê°€ ë©ì–´ë¦¬ìš”ì†Œ ì‹œì‘ë‹¨ì–´ê°€ ë˜ë²„ë¦¬ëŠ” ê²½ìš° ê·¸ ë’¤ toë¥¼ ì‹œì‘ë‹¨ì–´(.5)ë¡œ ìˆ˜ì •í•˜ê³ 
    #            youëŠ”(.5)ë¥¼ ì—†ì•° (ì˜ˆë¬¸ : I want you to succeed.)

        # âœ… ë‹¨ì–´ë©ì–´ë¦¬ ë§¨ ì• í† í° ì°¾ê¸°
        sorted_clause = sorted(clause_tokens, key=lambda x: x["idx"])
        first_token = sorted_clause[0]

        # ğŸ”¥ ë‹¨ì–´ë©ì–´ë¦¬ ë§¨ ì• ë‹¨ì–´ê°€ nsubjì¸ì§€ ì²´í¬
        if first_token.get("dep") == "nsubj":
            to_token = next((child for child in children if child.get("tag") == "TO"), None)
            if to_token:
                to_head_idx = to_token.get("head_idx")
                to_head_token = next((t for t in parsed if t["idx"] == to_head_idx), None)

                if to_head_token and to_head_token.get('dep') == "ccomp":
                    # ğŸ¯ í•µì‹¬: TOê°€ ì—°ê²°ëœ ccomp ì ˆì´ë©´ ë ˆë²¨ ì„¤ì •
                    to_token["level"] = current_level - 0.5
                    first_token["level"] = current_level - 1
                else:
                    # ğŸ¯ TO ì—†ê±°ë‚˜ ì¡°ê±´ ë¶ˆì¶©ì¡± ì‹œ, nsubjë§Œ .5 ë ˆë²¨
                    first_token["level"] = current_level - 0.5
            else:
                first_token["level"] = current_level - 0.5

            current_level += 1
            continue
    # rule_base_parse() í•¨ìˆ˜ì—ì„œëŠ” youì™€ toì˜ role1ì„ ì…ë ¥í•¨.

#######################################################################################################

        # âœ… ì—°ê²°ì–´ì—ëŠ” .5 ì¶”ê°€
        if dep == "acl":
            token["level"] = current_level - 0.5  # ì—°ê²°ì–´ëŠ” ë°”ë¡œ ì´ì „ ì ˆì—ì„œ ì´ì–´ì§
        else:
            # ì—°ê²°ì–´ í›„ë³´: ì ˆ ë²”ìœ„ ì• ë‹¨ì–´ ì¤‘ ì—°ê²°ì‚¬ ì—­í• 
            connector = min(clause_tokens, key=lambda x: x["idx"])
            connector["level"] = current_level - 0.5

        current_level += 1

    for i in range(len(clause_units) - 1):
        unit1 = clause_units[i]
        unit2 = clause_units[i + 1]

        first = unit1["indices"]
        second = unit2["indices"]

        if not first or not second:
            continue

        if second[0] < first[0] and second[-1] > first[-1]:
            # ì•ˆì€ ì ˆ +1
            for t in unit1["tokens"]:
                if t.get("level") is not None:
                    t["level"] += 1
                    print(f"[DEBUG ë””ë²„ê·¸11111] {current_level}")
            # ì•ˆê¸´ ì ˆ -1 (ê²¹ì¹˜ëŠ” ê²ƒ ë¹¼ê³ )
            for t in unit2["tokens"]:
                if t["idx"] not in first and t.get("level") is not None:
                    t["level"] -= 1
                    print(f"[DEBUG ë””ë²„ê·¸22222] {current_level}")


    # âœ… ìµœìƒìœ„ ì ˆ level=None â†’ level=0 ìœ¼ë¡œ ì„¤ì •
    for t in parsed:
        if t.get("level") is None:
            t["level"] = 0

    return parsed


# ëª©ì ë³´ì–´ë¥¼ ê³„ì¸µ ìœ ë°œ ìš”ì†Œë¡œ íƒœê¹…í•´ levelì´ ë°œìƒí•˜ëŠ” ê²ƒì— ëŒ€í•œ ì˜ˆì™¸ì²˜ë¦¬ í•¨ìˆ˜ì„
def is_valid_clause_trigger(token: dict) -> bool:
    """
    ì ˆ(clause) íŠ¸ë¦¬ê±°ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í† í°ì¸ì§€ íŒë³„í•©ë‹ˆë‹¤.

    ì˜ˆì™¸ ì¡°ê±´:
    - depê°€ clauseìš© dep (ccomp, xcomp, advcl ë“±)ê°€ ì•„ë‹˜
    - ë³´ì–´ ì—­í• ì¸ ê²½ìš° (object complement ë“±)
    í–¥í›„ ì¡°ê±´ì´ ë” ìƒê¸°ë©´ ì—¬ê¸°ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    dep = token.get("dep")
    role1 = token.get("role1")
    pos = token.get("pos") 

    if dep not in level_trigger_deps:
        return False

    if role1 in ["adjective object complement", "noun object complement"]:
        return False

    # âœ… ì˜ˆì™¸: ADJì¸ë° dep=advcl ì¸ ê²½ìš°ëŠ” ì ˆ ì•„ë‹˜
    if dep == "advcl" and pos == "ADJ":
        return False
    
    # í–¥í›„ ë” ì˜ˆì™¸ì¡°ê±´ì´ ìƒê¸°ë©´ ì—¬ê¸°ì— ì¶”ê°€
    return True

def repair_level_within_prepositional_phrases(parsed):
    """
    ì „ì¹˜ì‚¬(prep ë˜ëŠ” agent)ì˜ ëª©ì ì–´(pobj) ë ˆë²¨ì´ ë‹¤ë¥¼ ê²½ìš°
    ì „ì¹˜ì‚¬ì˜ level ê¸°ì¤€ìœ¼ë¡œ ë²”ìœ„ ë‚´ í† í°ë“¤ì„ ë³´ì •.
    ì˜ˆë¬¸) She is certain that he will arrive on time.
    """

    for prep in parsed:
        if prep.get("dep") not in {"prep", "agent"}:
            continue

        prep_level = prep.get("level")
        if prep_level is None:
            continue

        prep_idx = prep["idx"]

        # âœ… ëª¨ë“  í† í° ì¤‘ì—ì„œ pobj í›„ë³´ ì°¾ê¸° (children ì¡°ê±´ ì œì™¸)
        pobj_candidates = [
            t for t in parsed
            if t.get("dep") == "pobj" and t.get("head_idx") == prep_idx
        ]

        for pobj in pobj_candidates:
            pobj_level = pobj.get("level")

            if pobj_level == prep_level:
                continue  # ì´ë¯¸ ë™ì¼í•˜ë©´ ê±´ë„ˆëœ€

            # âœ… prep ~ pobj ì‚¬ì´ ë²”ìœ„ë¥¼ ì°¾ì•„ level ë³´ì •
            start = min(prep_idx, pobj["idx"])
            end = max(prep_idx, pobj["idx"])

            for t in parsed:
                if start <= t["idx"] <= end:
                    t["level"] = prep_level

    return parsed


def get_subclause_verbals_type(token, all_tokens):

    # 1ï¸âƒ£ ì¢…ì†ì ˆ (Subordinate Clause)
    if (
        token.get("pos") == "VERB" and
        token.get("dep") in {"ccomp", "xcomp", "advcl", "csubj"}
    ):
        children = [t for t in all_tokens if t.get("head_idx") == token["idx"]]
        has_sconj_marker = any(
            c.get("dep") in {"mark", "advmod"} and c.get("pos") == "SCONJ" and
            c.get("text", "").lower() in {
                "that", "if", "whether", "because", "although", "since",
                "when", "unless", "though", "as"
            }
            for c in children
        )
        if has_sconj_marker:
            return "subordinate_clause"

    # 2ï¸âƒ£ to ë¶€ì •ì‚¬
    if (
        token.get("pos") == "PART" and token.get("tag") == "TO" and token.get("dep") == "aux"
        and token.get("lemma", "").lower() == "to"
    ):
        to_idx = token["idx"]
        verb_after = next(
            (t for t in all_tokens if t["idx"] > to_idx and t.get("pos") == "VERB" and t.get("tag") == "VB"),
            None
        )
        if verb_after:
            return "to_infinitive"

    # 3ï¸âƒ£ bare infinitive (TO ì—†ì´ ë™ì‚¬ ì›í˜•)
    if (
        token.get("pos") == "VERB" and
        token.get("tag") == "VB" and
        not any(
            t.get("idx") == token["idx"] - 1 and
            t.get("text", "").lower() == "to" and
            t.get("tag") == "TO"
            for t in all_tokens
        )
    ):
        return "bare_infinitive"
    
    # 4ï¸âƒ£ ë™ëª…ì‚¬
    if (
        token.get("morph", {}).get("VerbForm") == "Ger" or
        (token.get("tag") == "VBG" and token.get("text", "").lower().endswith("ing"))
        # token.get("dep") in {"nsubj", "dobj", "obj", "pobj", "attr"}
    ):
        return "gerund"  # ë™ëª…ì‚¬

    # 5ï¸âƒ£ í˜„ì¬ë¶„ì‚¬
    if token.get("tag") == "VBG":
        verb_form = token.get("morph", {}).get("VerbForm")
        if verb_form != "Ger":
            if token.get("pos") == "VERB" and token.get("dep") in {
                "amod", "acl", "advcl", "xcomp", "ccomp", "conj"
            }:
                return "present_participle"

    # 6ï¸âƒ£ ê³¼ê±°ë¶„ì‚¬
    if token.get("tag") == "VBN" and token.get("pos") == "VERB":
        verb_form = token.get("morph", {}).get("VerbForm")
        if not verb_form or verb_form == "Part":
            return "past_participle"

    # 7ï¸âƒ£ reduced clause (ë¶„ì‚¬êµ¬ë¬¸)
    if (
        token.get("pos") == "VERB" and
        token.get("dep") in {"advcl", "amod"} and
        token.get("tag") in {"VBG", "VBN"}
    ):
        return "reduced_clause"

    return None


def get_chunks_partofspeech(token, all_tokens):

    form_type = get_subclause_verbals_type(token, all_tokens)
    dep = token.get("dep")

    if form_type == "subordinate_clause" and dep in {"nsubj", "csubj", "obj", "dobj"}:
        children = [t for t in all_tokens if t.get("head_idx") == token["idx"]]
        has_noun_sconj = any(
            c.get("pos") == "SCONJ" and
            c.get("dep") == "mark" and
            c.get("tag") == "IN" and
            c.get("text", "").lower() in {"that", "if", "whether"}  # ëª…ì‚¬ì ˆ ì „ìš© SCONJ
            for c in children
        )
        if has_noun_sconj:
            return "subclause_noun"
        
    if form_type == "subordinate_clause":
        children = [t for t in all_tokens if t.get("head_idx") == token["idx"]]
        has_adv_sconj = any(
            c.get("pos") == "SCONJ" and
            c.get("dep") in {"mark", "advmod"} and
            c.get("tag") in {"IN", "WRB"} and
            c.get("text", "").lower() in {
                "because", "since", "although", "when", "while", "if", "unless", "as", "though"
            }
            for c in children
        )
        if has_adv_sconj:
            return "subclause_adverb"


    if form_type == "to_infinitive":
        token["role2"] = "to infinitive"

        head_idx = token.get("head_idx")
        head_token = next((t for t in all_tokens if t["idx"] == head_idx), None)
        head_dep = head_token.get("dep") if head_token else None

        if head_dep in {"csubj"}:
            return "to.R_noun"
        elif head_dep in {"xcomp", "ccomp"}:
            return "to.R_noun.adj_dontcare"
        elif head_dep in {"relcl"}:
            return "to.R_adjective"
        elif head_dep in {"advcl"}:
            return "to.R_adverb"


    if form_type == "gerund":
        token["role2"] = "gerund"
        if token.get("dep") in {"nsubj", "csubj", "obj", "dobj", "pobj", "attr"}:
            return "R.ing_ger_noun"

    return None  # í•´ë‹¹ì‚¬í•­ ì—†ìœ¼ë©´ None


def assign_chunk_roles_and_drawsymbols(parsed):

    all_subject_complements = {
        "noun subject_complement", "adjective subject_complement"
    }

    line_length = memory["sentence_length"]
    symbols_by_level = memory["symbols_by_level"]

    # ê³„ì¸µì‹œì‘ìš”ì†Œ(level x.5ë‹¨ì–´)ê°€ ì•„ë‹ˆë©´ ë£¨í”„ ë¹ ì ¸ ë‚˜ê°
    for token in parsed:
        level = token.get("level")
        if not (isinstance(level, float) and level % 1 == 0.5):
            continue

        #ê³„ì¸µì‹œì‘ìš”ì†Œì˜ í—¤ë“œ ê°’ì´ ì—†ìœ¼ë©´ ë£¨í”„ ë¹ ì ¸ ë‚˜ê°
        head_idx = token.get("head_idx")
        head_token = next((t for t in parsed if t["idx"] == head_idx), None)
        if not head_token:
            continue

        chunks_pos = get_chunks_partofspeech(token, parsed)

        token_dep = token.get("dep")
        head_dep = head_token.get("dep")

        # ë¶€ì‚¬ë©ì–´ë¦¬ ë¨¼ì € í™•ì • : ë©ì–´ë¦¬ìš”ì†Œ ì²«ë‹¨ì–´ì˜ headì˜ depê°€ advclì´ê³ ,
        # chunks_pos == "subclause_adverb"ì´ë©´ role3ì— 'chunk_adverb_modifier'ê°’ ì…ë ¥

        if (chunks_pos == "subclause_adverb" and token_dep == "advcl" or head_dep == "advcl"):
            token["role3"] = "chunk_adverb_modifier"
            continue  # âœ… ë¶€ì‚¬ì ˆì´ë©´ ëª…ì‚¬ì ˆ ë¶„ê¸°ë¡œ ê±´ë„ˆëœ€

        chunks_partofspeech = get_chunks_partofspeech(token, parsed)

        # ì£¼ì–´ ëª…ì‚¬ë©ì–´ë¦¬ ê·¸ë‹¤ìŒ í™•ì • : ë©ì–´ë¦¬ìš”ì†Œ ì²«ë‹¨ì–´ì˜ headì˜ depê°€ csubj, nsubj, nsubjpassì´ê³ ,
        # is_nounchunk_trigger() í•¨ìˆ˜ì— ê±¸ë¦¬ë©´ role3ì— 'chunk_subject'ê°’ ì…ë ¥
        if (chunks_partofspeech and (token_dep in is_subject_deps) or (head_dep in is_subject_deps)):
            print(f"[DEBUG-chunks_partofspeech 02 in assign_chunk_roles_and_drawsymbols] {chunks_partofspeech}")
            token["role3"] = "chunk_subject"
            continue

        # ëª…ì‚¬ë©ì–´ë¦¬ íŒë‹¨ : 'ê³„ì¸µì‹œì‘ìš”ì†Œ' ë˜ëŠ” 'ê³„ì¸µì‹œì‘ìš”ì†Œì˜ í—¤ë“œ'ì˜ depê°€(ccomp, xcomp) ì´ê³ , 
        # ê³„ì¸µì‹œì‘ìš”ì†Œê°€ is_nounchunk_triggerì— ê±¸ë¦¬ë©´,
        print(f"[DEBUG-chunks_partofspeech 01 in assign_chunk_roles_and_drawsymbols {chunks_partofspeech}")

        if (
            (token_dep in {"ccomp", "xcomp"} or head_dep in {"ccomp", "xcomp"})
            and chunks_partofspeech
        ):
            if chunks_partofspeech == "to.R_noun":  
                token["role2"] = "to infinitive"

            if (
                (token_dep in {"xcomp"} or head_dep in {"xcomp"})
                and chunks_partofspeech == "R.ing_ger_noun"
            ):
                token["role2"] = "gerund"   # â˜œ í™•ì¸í•„ìš” ì•„ë˜ buildë¥¼ gerundë¡œ ì €ì¥í•¨
                                            # To be honest helps build trust.

            # ê³„ì¸µì‹œì‘ìš”ì†Œì˜ ìœ íš¨í•œ head ì°¾ì•„ì„œ headê°’ì´ ì—†ìœ¼ë©´ ë£¨í”„ ë¹ ì ¸ë‚˜ê°
            # toë¶€ì •ì‚¬(to infinitive)ì¸ ê²½ìš°ë§Œ headì˜ headë¡œ íƒ€ê³  ì˜¬ë¼ê°€ê¸°
            head2_token = (
                next((t for t in parsed if t["idx"] == head_token.get("head_idx")), None)
                if chunks_partofspeech in {"to.R_noun", "subclause_noun"}
                else head_token
            )
            if not head2_token:
                continue

            # ê³„ì¸µì‹œì‘ìš”ì†Œ headì˜ headì¸ ìƒìœ„ë™ì‚¬(head2) lemmaê°’ ì €ì¥
            head2_lemma = head2_token.get("lemma", "")

            # 1) ëª…ì‚¬ë©ì–´ë¦¬ í™•ì •í›„ ìƒìœ„ë™ì‚¬ê°€ beë™ì‚¬ì™€ LinkingVerbsì¸ë°,
            # ìƒìœ„ë™ì‚¬(í˜„í† í° í—¤ë“œì˜ í—¤ë“œ)ê°€ ì´ë¯¸ ëª…ì‚¬ë³´ì–´, í˜•ìš©ì‚¬ë³´ì–´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•Šì„ê²½ìš° ë³´ì–´ í™•ì •
            # ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ì˜ role1ì— 'noun subject complement'(ëª…ì‚¬ì£¼ì–´ë³´ì–´)ê°’ ì €ì¥
            if (
                head2_lemma in beVerbs or head2_lemma in notbeLinkingVerbs_onlySVC
            ) and not any(
                c.get("role1") in {"all_subject_complements"}
                for c in head2_token.get("combine", [])
            ):
                token["role1"] = "noun subject complement"

            # 2) ìƒìœ„ë™ì‚¬ê°€ dativeVerbsì¼ë•Œ ìƒìœ„ë™ì‚¬level ë‹¨ì–´ë“¤ì˜ role1ì— objedct, indirect objectê°€ ìˆìœ¼ë©´
            # ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ì˜ role1ì— 'direct object'(ì§ì ‘ëª©ì ì–´)ê°’ ì €ì¥
            # ì•„ë‹ˆë©´ role1ì— 'object'(ëª©ì ì–´)ê°’ ì €ì¥
            elif head2_lemma in dativeVerbs:
                current_level = int(level)  # x.5 -> x
                # í˜„ì¬ ë ˆë²¨ì˜ í† í°ë“¤
                level_tokens = [t for t in parsed if int(t.get("level", -1)) == current_level]
                has_obj_or_iobj = any(
                    t.get("role1") in {"object", "indirect object"} for t in level_tokens
                )
                if has_obj_or_iobj:
                    token["role1"] = "direct object"
                else:
                    token["role1"] = "object"

            # ì• ëª¨ë“  ì¡°ê±´ì— ì•ˆê±¸ë¦¬ë©´ ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ì˜ rele1ì— 'object'(ëª©ì ì–´) ê°’ ì €ì¥
            else:
                token["role3"] = "chunk_not_decide"
                token["role1"] = "object"


        # âœ… # í˜„í† í°ì˜ headì˜ childrenë“¤ ëª¨ìŒ (ë í† í° ì°¾ê¸° + ì‹œì‘ í† í° info)
        children_tokens = [child for child in parsed if child.get("head_idx") == head_idx]
        children_tokens.append(head_token)              # í˜„í† í°ì˜ head tokenê¹Œì§€ ë³‘í•©
        children_tokens.sort(key=lambda x: x["idx"])    # ë‹¨ì–´ë“¤ì˜ ìˆœì„œë¥¼ ì™¼ìª½ë¶€í„° ì •ë ¬í•¨
        end_token = children_tokens[-1]

        # ë í† í°ì´ êµ¬ë‘ì (. ! ?)ì´ë©´ ê·¸ ì• í† í° ì‚¬ìš©
        if (
            end_token.get("pos") == "PUNCT" and
            end_token.get("text") in {".", "!", "?"} and
            len(children_tokens) >= 2
        ):
            end_token = children_tokens[-2]

        start_idx = token["idx"]
        end_idx = end_token["idx"]
        end_idx_adjusted = end_idx + len(end_token.get("text", "")) - 1 # ëë‹¨ì–´ ëê¸€ì ì¸ë±ìŠ¤ ê³„ì‚°
        int_level = int(level) # .5ìš”ì†Œì´ì§€ë§Œ ë©ì–´ë¦¬ ëí‘œì‹œë¥¼ ìƒìœ„ê³„ì¸µì— ë§ì¶”ì–´ ê·¸ë ¤ì•¼ í•˜ë¯€ë¡œ ì†Œìˆ˜ì (.5)ë²„ë¦¼

        role1 = token.get("role1")
        line = symbols_by_level.setdefault(int_level, [" " for _ in range(line_length)])

        chunk_end_mark = None

        # âœ… ê¸°ë³¸ ì‹¬ë³¼
        if role1 in {"noun subject complement", "object", "indirect object", "direct object",
                     "noun object complement"}:
            chunk_end_mark = "]"

#        if 0 <= start_idx < line_length:
#            line[start_idx] = left
        if chunk_end_mark and (0 <= end_idx_adjusted < line_length) and token.get("pos") != "VERB":
            line[end_idx_adjusted] = chunk_end_mark

        # âœ… to infinitive â†’ to.o...R
        if token.get("role2") == "to infinitive":
            verb_token = next(
                (t for t in parsed
                 if t["idx"] > start_idx and
                    int(t.get("level", 0)) == int_level + 1 and
                    t.get("pos") == "VERB"),
                None
            )
            if verb_token:
                verb_idx = verb_token["idx"]
                verb_end = verb_idx + len(verb_token["text"]) - 1
                line2 = symbols_by_level.setdefault(int_level + 1, [" " for _ in range(line_length)])
                if 0 <= start_idx < line_length: line2[start_idx] = "t"
                if 0 <= start_idx + 1 < line_length: line2[start_idx + 1] = "o"
                for i in range(start_idx + 2, verb_end):
                    if 0 <= i < line_length and line2[i] == " ":
                        line2[i] = "."
                if 0 <= verb_end < line_length: line2[verb_end] = "R"

        # âœ… gerund â†’ R...ing
        if token.get("role2") == "gerund":
            verb_token = next(
                (t for t in parsed
                 if t["idx"] >= start_idx
                    and (t.get("level") == level or int(t.get("level", 0)) == int_level + 1)
                    and t.get("pos") == "VERB"),
                None
            )
            if verb_token:
                verb_idx = verb_token["idx"]
                verb_end = verb_idx + len(verb_token["text"]) - 1
                line2 = symbols_by_level.setdefault(int_level + 1, [" " for _ in range(line_length)])
                if 0 <= start_idx < line_length: line2[start_idx] = "R"
                for i in range(start_idx + 1, verb_end-2):
                    if 0 <= i < line_length and line2[i] == " ":
                        line2[i] = "."
                if 0 <= verb_end < line_length: line2[verb_end-2] = "i"
                if 0 <= verb_end < line_length: line2[verb_end-1] = "n"
                if 0 <= verb_end < line_length: line2[verb_end] = "g"

    return parsed
    

def apply_subject_adverb_chunk_range_symbol(parsed):
    """
    role3=chunk_subjectì¸ í† í°ì„ ê¸°ì¤€ìœ¼ë¡œ
    í•´ë‹¹ ì ˆ(start_idx ~ end_idx) ë²”ìœ„ì— [ ] ì‹¬ë³¼ ë¶€ì—¬
    """
    line_length = memory["sentence_length"]
    symbols_by_level = memory["symbols_by_level"]

    for token in parsed:
        role3 = token.get("role3")
        if not role3:
            continue

        level = token.get("level")
        if level is None:
            continue

        line = symbols_by_level.setdefault(int(level), [" " for _ in range(line_length)])
        chunks_partofspeech = get_chunks_partofspeech(token, parsed)

        start_idx = token["idx"]
        head_idx = token.get("head_idx")
        head_token = next((t for t in parsed if t["idx"] == head_idx), None)

        if not head_token:
            continue

        children_tokens = [child for child in parsed if child.get("head_idx") == head_idx]
        children_tokens.append(head_token)
        if not children_tokens:
            continue

        children_tokens.sort(key=lambda x: x["idx"])

        end_token = children_tokens[-1]


        # ë™ëª…ì‚¬ì˜ ê²½ìš° gerundì˜ í—¤ë“œì˜ ìì‹ì„ ì´ìš©í•´ì•¼í•˜ëŠ”ë° gerund ë²”ìœ„ ë°–ì˜ ë‹¨ì–´ê¹Œì§€ í¬í•¨ë˜ë²„ë¦¼
        # ì´ ë¶€ë¶„ì€ ê·¸ ê²½ìš°ì˜ ë³´ì •ì„. ì˜ˆë¬¸) Watching movies affects my sleep.
        if role3 == "chunk_subject" and chunks_partofspeech == "R.ing_ger_noun":
            for i, child in enumerate(children_tokens):
                if child.get("pos") == "VERB" and child["idx"] > token["idx"]:
                    if i > 0:
                        end_token = children_tokens[i - 1]
                    break

        # ë™ëª…ì‚¬ë©ì–´ë¦¬ê°€ ì£¼ì–´ì¸ ê²½ìš° ë©ì–´ë¦¬ ëì´ ë§ˆì¹¨í‘œ ë‚˜ì˜¬ì¼ ì—†ìŒ(ì´ ì†ŒìŠ¤ëŠ” í•„ìš”ì—†ì–´ ë³´ì„)
        # if end_token.get("pos") == "PUNCT" and len(children_tokens) >= 2:
        #    end_token = children_tokens[children_tokens.index(end_token) - 1]

        end_idx = end_token["idx"]
        end_idx_adjusted = end_idx + len(end_token["text"]) - 1

        # âœ… role3ì— ë”°ë¼ ì‹¬ë³¼ ë‹¤ë¥´ê²Œ
        if role3 == "chunk_subject":
            left, right = "[", "]"
        elif role3 == "chunk_adverb_modifier":
            left, right = "<", ">"
        else:
            continue

        if 0 <= start_idx < line_length:
            line[start_idx] = left
        if 0 <= end_idx_adjusted < line_length:
            line[end_idx_adjusted] = right


def NounChunk_combine_apply_to_upverb(parsed):
    """
    ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ role2ê°€ object / direct object / noun subject complementì¼ë•Œ
    ìƒìœ„ ë™ì‚¬ì˜ comnbinì— role2ë¥¼ ì…ë ¥í•´ì£¼ëŠ” í•¨ìˆ˜
    """
    for token in parsed:
        role2 = token.get("role2")
        # ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ì˜ role2ê°€ ì´ 3ê°œì¼ë•Œë§Œ ì•„ë˜ ì†ŒìŠ¤ ì²˜ë¦¬
        if role2 not in {
            "object", "direct object",
             "noun subject complement", "noun object complement"
        }:
            continue

        # ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ì˜ head(ë³´í†µ ë™ì‚¬)ì˜ depê°€ ccomp(ì¢…ì†ì ‘ì†ì‚¬)ì¼ë•Œë§Œ ì•„ë˜ ì†ŒìŠ¤ ì²˜ë¦¬
        head_idx = token.get("head_idx")
        head_token = next((t for t in parsed if t["idx"] == head_idx), None)
        if not head_token or head_token.get("dep") not in {"ccomp", "xcomp"}:
            continue

        # ëª…ì‚¬ë©ì–´ë¦¬ ì²«ë‹¨ì–´ì˜ headì˜ head(ìƒìœ„ ë™ì‚¬ head2)ê°€ ìˆìœ¼ë©´ ì•„ë˜ ì†ŒìŠ¤ ì²˜ë¦¬ë¦¬
        head2_idx = head_token.get("head_idx")
        head2_token = next((t for t in parsed if t["idx"] == head2_idx), None)
        if not head2_token:
            continue
        if "combine" not in head2_token or not head2_token["combine"]:
            head2_token["combine"] = []

        # ğŸ”¥ ìƒìœ„ ë™ì‚¬ì˜ combineì— ìœ„ role2 3ê°œì¤‘ 1ê°œ(text, role2ê°’, idxê°’) ì…ë ¥
        head2_token["combine"].append({
            "text": token["text"],
            "role2": role2,
            "idx": token["idx"]
        })


# ì•„ë¬´ ì‹¬ë³¼ë„ ì•ˆ ì°íŒ ì¤„ì´ë©´ memoryì—ì„œ ì•„ì˜ˆ ì œê±°
def clean_empty_symbol_lines():
    """
    memory["symbols_by_level"] ì¤‘ ë‚´ìš©ì´ ì „ë¶€ ê³µë°±ì¸ ì¤„ì€ ì œê±°í•œë‹¤.
    """
    keys_to_remove = []
    for level, line in memory["symbols_by_level"].items():
        if all(c == " " for c in line):
            keys_to_remove.append(level)

    for level in keys_to_remove:
        del memory["symbols_by_level"][level]


# ë™ì‚¬ë©ì–´ë¦¬(verb chain) í•˜ë‚˜ ë°›ì•„ì„œ ì‹œì œ/ìƒ/íƒœ ë¶„ì„í•˜ê³  symbol_map ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
def set_verbchunk_attributes(chain):

    symbol_map = {}
    aspect = []
    voice = None

    if not chain:
        return symbol_map, aspect, voice

    verb_attr = memory["symbols_all"]["verb_attr"]

    # ë§¨ ì• í† í°
    first = chain[0]
    first_lemma = first.get("lemma", "").lower()
    first_pos = first.get("pos")
    first_dep = first.get("dep")
    first_tag = first.get("tag")

    # âœ… P1. ë§¨ì• modal ì—¬ë¶€
    if first_pos == "AUX" and first_dep == "aux" and first_tag == "MD":
        if first_lemma in modalVerbs_present:
            symbol_map[first["idx"]] = verb_attr["present tense"]
        elif first_lemma in modalVerbs_past:
            symbol_map[first["idx"]] = verb_attr["past tense"]

    # âœ… P2. ì¤‘ê°„ ì¡°ë™ì‚¬ë“¤ ì²˜ë¦¬
    for t in chain:
        pos = t.get("pos")
        dep = t.get("dep")
        tag = t.get("tag")
        text = t.get("text", "").lower()

        # aux, auxpassë§Œ ì¡°ë™ì‚¬
        if not (pos == "AUX" and dep in {"aux", "auxpass"}):
            break  # ì¡°ë™ì‚¬ ì•„ë‹ˆë©´ (ì¦‰ ë³¸ë™ì‚¬) -> P3ë¡œ

        # ì¡°ë™ì‚¬ ì‹œì œ (fin)
        verbform = t.get("morph", {}).get("VerbForm", "")
        tense = t.get("morph", {}).get("Tense", "")

        if verbform == "Fin":
            if tag in {"VBP", "VBZ"} or tense == "Pres":
                symbol_map[t["idx"]] = verb_attr["present tense"]
            elif tag == "VBD" or tense == "Past":
                symbol_map[t["idx"]] = verb_attr["past tense"]

        # ì™„ë£Œ, ì§„í–‰
        if text == "been" and tag == "VBN":
            symbol_map[t["idx"]] = verb_attr["perfect aspect"]
            if "perfect" not in aspect:
                aspect.append("perfect")
        elif text == "being" and tag == "VBG":
            symbol_map[t["idx"]] = verb_attr["progressive aspect"]
            if "progressive" not in aspect:
                aspect.append("progressive")

        # ì›í˜• ì¡°ë™ì‚¬(VB, Inf)ëŠ” ì•„ë¬´ê²ƒë„ ì•ˆì°ê³  continue
        if tag == "VB" and verbform == "Inf":
            continue

    # âœ… P3. ë³¸ë™ì‚¬ ì²˜ë¦¬
    last = chain[-1]
    pos = last.get("pos")
    dep = last.get("dep")
    tag = last.get("tag")
    lemma = last.get("lemma", "").lower()

    if dep in level_trigger_deps or dep == "root":
        verbform = last.get("morph", {}).get("VerbForm", "")
        tense = last.get("morph", {}).get("Tense", "")

        if verbform == "Fin":
            if tag in {"VBP", "VBZ"} or tense == "Pres":
                symbol_map[last["idx"]] = verb_attr["present tense"]
            elif tag == "VBD" or tense == "Past":
                symbol_map[last["idx"]] = verb_attr["past tense"]


        # ì™„ë£Œ/ìˆ˜ë™/ì§„í–‰
        if tag == "VBN":
            # ì™¼ìª½ìœ¼ë¡œ AUX aux/auxpass ì°¾ì•„ì•¼ í•´
            for prev in reversed(chain[:-1]):
                if prev.get("pos") != "AUX":
                    continue
                prev_dep = prev.get("dep")
                prev_lemma = prev.get("lemma", "").lower()
                if prev_dep == "aux" and prev_lemma == "have":
                    symbol_map[last["idx"]] = verb_attr["perfect aspect"]
                    if "perfect" not in aspect:
                        aspect.append("perfect")
                    break
                elif prev_dep == "auxpass" and prev_lemma == "be":
                    symbol_map[last["idx"]] = verb_attr["passive voice"]
                    voice = "passive"
                    break

        elif tag == "VBG":
            symbol_map[last["idx"]] = verb_attr["progressive aspect"]
            if "progressive" not in aspect:
                aspect.append("progressive")

    return symbol_map, aspect, voice

# ë¬¸ì¥ì˜ ì „ì²´ parsed ê²°ê³¼ë¥¼ ë°›ì•„ ë™ì‚¬ë©ì–´ë¦¬ë³„ ì‹œì œ/ìƒ/íƒœ ë¶„ì„.
def set_allverbchunk_attributes(parsed):
    memory["verb_attribute_by_chain"] = []
    memory["verb_attribute"] = {}
    sentence_len = memory["sentence_length"]

    chains = []
    current_chain = []
    last_level = None

    # ë™ì‚¬ë©ì–´ë¦¬ ë¶„ë¦¬
    for token in parsed:
        level = token.get("level", 0)

        if last_level is None:
            last_level = level

        # ë“±ìœ„ì ‘ì†ì‚¬ê°€ ë‚˜ì˜¤ë©´ ë™ì‚¬ë©ì–´ë¦¬ ëŠìŒ (ì¢…ì†ì ‘ì†ì‚¬ëŠ” level ë°œìƒ ë¶€ë¶„ì—ì„œ ì²˜ë¦¬ ê°€ëŠ¥)
        if (
            token.get("dep") in {"cc"} and token.get("pos") in {"CCONJ", "CONJ"}
        ):
            if current_chain:
                chains.append(current_chain)
                current_chain = []
            last_level = level

        # level ë°”ë€” ë•Œ ëŠê¸°
        if last_level is not None and level != last_level:
            if current_chain:
                chains.append(current_chain)
                current_chain = []
            last_level = level

        # âœ… AUX, VERB ì¶”ê°€
        if token["pos"] in {"AUX", "VERB"}:
            current_chain.append(token)

    if current_chain:
        chains.append(current_chain)

    all_symbol_maps = {}

    # ê° chain ë¶„ì„
    for chain in chains:
        if not chain:
            continue
        
        first = chain[0]
        last = chain[-1]

        symbol_map, aspect, voice = set_verbchunk_attributes(chain)

        # ì €ì¥ (ë””ë²„ê¹…ìš©, í™•ì¥ìš©)
        memory["verb_attribute_by_chain"].append({
            "aspect": aspect,
            "voice": voice,
            "main_verb": chain[-1]["text"],
            "verb_chain": [t["text"] for t in chain],
            "symbol_map": symbol_map
        })

        all_symbol_maps.update(symbol_map)

    memory["verb_attribute"] = {
        "symbol_map": all_symbol_maps,
        "main_verb": last["text"],
        "aspect": aspect,
        "voice": voice
    }

    print(chains)

# â— GPT í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜
def spacy_parsing_backgpt(sentence: str, force_gpt: bool = False):

#    memory["used_gpt"] = False  # âœ… ê¸°ë³¸ê°’: GPT ë¯¸ì‚¬ìš©
    doc = nlp(sentence)

    prompt = f"""

"""
    # spaCyì—ì„œ í† í° ë°ì´í„° ì¶”ì¶œ
    tokens = []
    for token in doc:
        morph = token.morph.to_dict()
        tokens.append({
            "idx": token.idx, "text": token.text, "pos": token.pos_,"tag": token.tag_,
            "dep": token.dep_.lower(), "head": token.head.text, "head_idx": token.head.idx,
            "tense": morph.get("Tense"), "aspect": morph.get("Aspect"), "voice": morph.get("Voice"), "form": morph.get("VerbForm"),
            "morph": morph, "lemma": token.lemma_, "is_stop": token.is_stop,
            "is_punct": token.is_punct, "is_alpha": token.is_alpha, "ent_type": token.ent_type_,
            "is_title": token.is_title, "children": [child.text for child in token.children]
        })

    # ê·œì¹™ ê¸°ë°˜ íŒŒì‹±
    parsed = rule_based_parse(tokens)

    # âœ… ë³´ì–´ í˜•ìš©ì‚¬ ë³´ì •: ADJì¸ë° objectë¡œ ëœ ê²½ìš°
    parsed = assign_adj_object_complement_when_compound_object(parsed)

    # âœ… ë³´ì–´ ê¸°ì¤€ìœ¼ë¡œ objectë¥¼ ë³µì› (compoundì¸ ê²½ìš° ë“±)
    parsed = repair_object_from_complement(parsed)

    # âœ… NEW: advcl+ADJ ë³´ì–´ ë³´ì •
    parsed = assign_adj_complement_for_advcl_adjective(parsed)

    # SVOO ê´€ë ¨ ë³´ì •(indirect object roleë§Œ ìˆëŠ” ê²½ìš°)
    parsed = recover_direct_object_from_indirect(parsed)


    # level ë¶„ê¸° ì „íŒŒ
    parsed = assign_level_trigger_ranges(parsed)

    # âœ… ìš”ê¸°! ëª¨ë“  ë³´ì • ëë‚œ í›„ì— combine ì¶”ë¡ 
    for t in parsed:
        combine = guess_combine(t, parsed)
        if combine:
            t["combine"] = combine

    # ì¡°ê±´: ê·œì¹™ ê¸°ë°˜ ì‹¤íŒ¨í•˜ê±°ë‚˜, ê°•ì œë¡œ GPT ì‚¬ìš© ìš”ì²­
    if not parsed or force_gpt:
        memory["used_gpt"] = True  # âœ… GPT fallback ì‚¬ìš©ëœ ê²½ìš°
        # GPT íŒŒì‹± í˜¸ì¶œ
        prompt = gpt_parsing_withprompt(tokens)  # ì•„ë˜ 2ë‹¨ê³„ì—ì„œ ë§Œë“¤ ì˜ˆì •

        try:
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are an expert sentence analyzer."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=500
            )
            content = response.choices[0].message.content
            return json.loads(content)
        except Exception as e:
            print("[ERROR] GPT parsing failed:", e)
            print("[RAW CONTENT]", content if 'content' in locals() else '[No response]')
            return []

    assign_chunk_roles_and_drawsymbols(parsed)  # â˜…â˜…â˜…â˜… ìœ„ì˜ assign_level_trigger_ranges() í•¨ìˆ˜ ìœ„ë¡œ ê°ˆ ìˆ˜ ì—†ë‹¤.
                                                # ê·¸ë˜ì„œ guess_combine_second()ë¥¼ í•œë²ˆ ë” í˜¸ì¶œí•œë‹¤.

    # âœ… ğŸ“ level ë³´ì •: prep-pobj ë ˆë²¨ í†µì¼
    parsed = repair_level_within_prepositional_phrases(parsed)

    parsed = guess_combine_second(parsed)

    set_allverbchunk_attributes(parsed)

    return parsed


# GPT API Parsing(with í”„ë¡¬í”„íŠ¸)ì„ ì´ìš©í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
def gpt_parsing_withprompt(tokens: list) -> str:
    token_lines = []
    for t in tokens:
        token_lines.append(
            f"â— idx({t['idx']}), text({t['text']}), pos({t['pos']}), tag({t['tag']}), dep({t['dep']}), head({t['head']})"
        )
    token_block = "\n".join(token_lines)

    prompt = f"""
Given the following tokenized and POS-tagged English sentence, analyze its syntactic structure.

Token Info:
{token_block}

Return a JSON list with each token's role in the sentence.
Each item must have: idx, text, role1, role2, role3, and optionally combine/level.

If unsure, return best-guess. Do not return explanations, just the JSON.
"""
    return prompt.strip()


# â— ì €ì¥ê³µê°„ ì´ˆê¸°í™”
def init_memorys (sentence: str):
#    memory["characters"] = list(sentence)        # charactersì— sentenceì˜ ê¸€ì í•œê¸€ìì”© ì±„ìš°ê¸°
    memory["symbols_by_level"] = {}  # ë¬¸ì¥ë§ˆë‹¤ ìƒˆë¡œ ì´ˆê¸°í™”
    memory["sentence_length"] = len(sentence)  # ë„ì‹ ê¸¸ì´ ì¶”ì ìš© (ì¤„ ê¸¸ì´ í†µì¼)


def lookup_symbol(name):
    name = name.lower()
    for symbol_category in symbols_all.values():
        for key, value in symbol_category.items():
            if key.lower() == name:
                return value
    return None

# â— symbols ë©”ëª¨ë¦¬ì— ì‹¬ë³¼ë“¤ ì €ì¥í•˜ê¸°
def apply_symbols(parsed):
    symbols_by_level = memory["symbols_by_level"]
    line_length = memory["sentence_length"]

    for item in parsed:
        idx = item.get("idx", -1)
        role1 = item.get("role1", "") or ""
        role2 = item.get("role2", "") or ""
        level = item.get("level")

        if idx < 0 or level is None:
            continue

        symbol1 = lookup_symbol(role1)
        symbol2 = lookup_symbol(role2)

        # âœ… 1. role1: ì •ìˆ˜ ë ˆë²¨ì—ë§Œ ì°ê¸°
        levels_role1 = [int(level)]  # <--- ì—¬ê¸° ìˆ˜ì •
        for lvl in levels_role1:
            line = symbols_by_level.setdefault(lvl, [" " for _ in range(line_length)])
            if 0 <= idx < len(line) and line[idx] == " " and symbol1:
                line[idx] = symbol1

        # âœ… 2. role2: (0.5 ë ˆë²¨ ë‹¨ì–´ì—ë§Œ)
        if isinstance(level, float) and (level % 1 == 0.5):
            lvl_role2 = int(level) + 1
            line2 = symbols_by_level.setdefault(lvl_role2, [" " for _ in range(line_length)])
            if 0 <= idx < len(line2) and line2[idx] == " " and symbol2:
                line2[idx] = symbol2

    # â¬‡ï¸ combine ì—°ê²°ì„ ì„ _ ë¡œ ê·¸ë ¤ì¤Œ!
    for item in parsed:
        combine = item.get("combine")
        level = item.get("level")
        idx1 = item.get("idx")

        if not combine or level is None:
            continue

        for comb in combine:
            idx2 = comb.get("idx")
            if idx2 is None:
                continue

            lvl = int(level + 0.5)

            line = symbols_by_level.setdefault(lvl, [" " for _ in range(line_length)])
            start = min(idx1, idx2)
            end = max(idx1, idx2)

            for i in range(start + 1, end):
                if line[i] == " ":
                    line[i] = "_"

    return parsed


# ì²˜ìŒ ë‚˜ì˜¤ëŠ” ì¡°ë™ì‚¬ì™€ ë³¸ë™ì‚¬ ì‚¬ì´ë¥¼ .(ì )ìœ¼ë¡œ ì—°ê²° ì‹œì¼œì¤Œ, ë ˆë²¨ ìˆœíšŒí•˜ë©°(ë‹¤ë¥¸ ë ˆë²¨ê°„ ì—°ê²°í• ì¼ ì—†ìŒ), ê¸°ì¡´ ë„í˜• ìˆìœ¼ë©´ ì•ˆì°ìŒ
def apply_aux_to_mverb_bridge_symbols_each_levels(parsed, sentence):

    for modal_token in [t for t in parsed if t["pos"] == "AUX" and t["dep"] in {"aux", "auxpass"}]:
        level = modal_token.get("level")
        if level is None:
            continue

        line = memory["symbols_by_level"].get(level)
        if not line:
            continue

        modal_idx = modal_token["idx"]

        # âœ… ì¡°ë™ì‚¬ ì´í›„ì— ë‚˜ì˜¤ëŠ” ì²« ë²ˆì§¸ ë³¸ë™ì‚¬(verb role)
        verb_token = next(
            (t for t in parsed
             if t.get("role1") == "verb"
             and t.get("level") == level
             and t["idx"] > modal_idx),
            None
        )
        if not verb_token:
            continue

        verb_idx = verb_token["idx"]
        start, end = sorted([modal_idx, verb_idx])

        # âœ… ì˜ë¬¸ë¬¸ íŒë‹¨
        has_subject_between = any(
            t.get("role1") == "subject" and start < t["idx"] < end
            for t in parsed
        )

        if has_subject_between:
            if line[modal_idx] == " ":
                line[modal_idx] = "âˆ©"
        else:
            if line[modal_idx] == " ":
                line[modal_idx] = "."

        for i in range(start + 1, end):
            if line[i] == " ":
                line[i] = "."


# ë™ì¼ë ˆë²¨, ê°™ì€ ì ˆì— ë™ì‚¬ê°€ ì—¬ëŸ¬ê°œ ë³‘ë ¬ ë‚˜ì—´ëœ ê²½ìš° ë™ì‚¬ë©ì–´ë¦¬ ì²˜ìŒ ìš”ì†Œì™€ ëìš”ì†Œë¥¼ .(ì )ìœ¼ë¡œ ì±„ì›Œì¤Œ
def draw_dot_bridge_across_verb_group(parsed):
    line_length = memory["sentence_length"]
    symbols_by_level = memory["symbols_by_level"]
    visited = set()

    for token in parsed:
        # âœ… role ì—†ì´ë„ ë™ì‚¬ë©´ ì ì„  ì—°ê²° ëŒ€ìƒ!
        if token.get("pos") not in {"AUX", "VERB"}:
            continue

        dep = token.get("dep", "").lower()
        if dep not in {"root", "conj", "xcomp", "ccomp"}:
            continue

        level = token.get("level")
        if level is None:
            continue

        idx1 = token["idx"]
        idx2 = None

        for t in parsed:
            if (
                t["idx"] > idx1 and
                t.get("level") == level and
                t.get("pos") in {"VERB", "AUX"} and
                t.get("dep", "").lower() in {"root", "conj", "xcomp", "ccomp"}
            ):
                has_subject_between = any(
                    s.get("role1") == "subject" and
                    s.get("level") == level and
                    idx1 < s["idx"] < t["idx"]
                    for s in parsed
                )
                if has_subject_between:
                    break
                idx2 = t["idx"]

        if idx2 and (idx1, idx2) not in visited:
            visited.add((idx1, idx2))
            line = symbols_by_level.setdefault(level, [" " for _ in range(line_length)])
            for i in range(idx1 + 1, idx2):
                if line[i] == " ":
                    line[i] = "."


# â— memory["symbols"] ë‚´ìš©ì„ ì¶œë ¥í•˜ê¸° ìœ„í•´ ë§Œë“  í•¨ìˆ˜
def symbols_to_diagram(sentence: str):
    output_lines = []

    line_length = memory["sentence_length"]
    parsed = memory.get("parsed")

    # âœ… ìƒˆ ë°©ì‹ìœ¼ë¡œ ì‹œì œ/ìƒ/íƒœ symbol map ì¶œë ¥
    tav_line = [" " for _ in range(line_length)]
    symbol_map = memory.get("verb_attribute", {}).get("symbol_map", {})
    for idx, symbol in symbol_map.items():
        if 0 <= idx < line_length:
            tav_line[idx] = symbol
    output_lines.append("".join(tav_line))  # â† ì²« ì¤„ë¡œ ì¶œë ¥

    # âœ… ë¬¸ì¥ í…ìŠ¤íŠ¸ ì¤„
    output_lines.append(sentence)

    # âœ… bridge(âˆ©) ë° â—‹â–¡ ì‹¬ë³¼ ì¶œë ¥
    if parsed:
        apply_aux_to_mverb_bridge_symbols_each_levels(parsed, sentence)

#   clean_empty_symbol_lines()

    for level in sorted(memory["symbols_by_level"]):
        output_lines.append(''.join(memory["symbols_by_level"][level]))

    draw_dot_bridge_across_verb_group(parsed)

    return '\n'.join(output_lines)


def guess_combine_second(parsed):
    for token in parsed:
        combine = guess_combine(token, parsed)
        if combine:
            token["combine"] = combine
    return parsed


def t(sentence: str):
    print(f"\nğŸ“˜ Sentence: {sentence}")

    # âœ… ë©”ëª¨ë¦¬ ë¨¼ì € ì´ˆê¸°í™” (ë¬¸ì¥ ê¸¸ì´ ê¸°ë°˜ ì„¤ì • í¬í•¨)
    init_memorys(sentence)

    # âœ… spaCy íŒŒì‹± + ì—­í•  ë¶„ì„
    parsed = spacy_parsing_backgpt(sentence)
    memory["parsed"] = parsed

#    if memory.get("used_gpt"):
#        print("âš ï¸ GPTê°€ íŒŒì‹±ì— ê°œì…í–ˆìŒ (ì†ë„ ëŠë¦´ ìˆ˜ ìˆìŒ)")
#    else:
#        print("âœ… spaCy ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ íŒŒì‹± ì™„ë£Œ")

   # NounChunk_combine_apply_to_upverb(parsed)
    apply_symbols(parsed)
    apply_subject_adverb_chunk_range_symbol(parsed)
    draw_dot_bridge_across_verb_group(parsed)

    # âœ… morph ìƒì„¸ ì¶œë ¥
    print("\nğŸ“Š Full Token Info with Annotations:")
    print(nlp.path)
    doc = nlp(sentence)
    for token in doc:
        morph = token.morph.to_dict()
        idx = token.idx
        text = token.text
        role1 = next((t.get("role1") for t in parsed if t["idx"] == idx), None)
        role2 = next((t.get("role2") for t in parsed if t["idx"] == idx), None)
        role3 = next((t.get("role3") for t in parsed if t["idx"] == idx), None)

        combine = next((t.get("combine") for t in parsed if t["idx"] == idx), None)
        level = next((t.get("level") for t in parsed if t["idx"] == idx), None)

        combine_str = (
            "[" + ", ".join(
                f"{c['text']}({c['idx']}):" + (
                    f"{c['role1']}" if 'role1' in c else ""
                ) + (
                    f"/{c['role2']}" if 'role2' in c else ""
                )
                for c in combine
            ) + "]"
            if combine else "None"
        )

        child_texts = [child.text for child in token.children]

        print(f"â— idx({idx}), text({text}), role1({role1}), role2({role2}), role3({role3}), combine({combine_str})")
        print(f"  level({level}), POS({token.pos_}), DEP({token.dep_}), TAG({token.tag_}), HEAD({token.head.text})")
        print(f"  lemma({token.lemma_}), is_stop({token.is_stop}), is_punct({token.is_punct}), is_title({token.is_title})")
        print(f"  morph({morph})")
        print(f"  children({child_texts})")
        print("")
        
    # âœ… ë„ì‹ ì¶œë ¥
    print("ğŸ›  Diagram:")
    print(symbols_to_diagram(sentence))


# ë¬¶ìŒ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def t1(sentence: str):
    # âœ… ë©”ëª¨ë¦¬ ë¨¼ì € ì´ˆê¸°í™” (ë¬¸ì¥ ê¸¸ì´ ê¸°ë°˜ ì„¤ì • í¬í•¨)
    init_memorys(sentence)

    # âœ… spaCy íŒŒì‹± + ì—­í•  ë¶„ì„
    parsed = spacy_parsing_backgpt(sentence)
    memory["parsed"] = parsed
    # âœ… ë„ì‹í™” ë° ì¶œë ¥
    chunk_info_list = assign_chunk_role(parsed)
    NounChunk_combine_apply_to_upverb(parsed)
    apply_subject_adverb_chunk_range_symbol(parsed)
    apply_symbols(parsed)
    apply_chunk_symbols_overwrite(chunk_info_list)
    draw_dot_bridge_across_verb_group(parsed)
    print("ğŸ›  Diagram:")
    print(symbols_to_diagram(sentence))



# â— ëª¨ë“ˆ ì™¸ë¶€ ì‚¬ìš©ì„ ìœ„í•œ export
__all__ = [
    "rule_based_parse",
    "guess_role",
    "assign_noun_complement_for_SVOC_noun_only",
    "repair_object_from_complement",
    "guess_combine",
    "assign_level_trigger_ranges",
    "spacy_parsing_backgpt",
    "gpt_parsing_withprompt",
    "init_memorys",
    "apply_symbols",
    "symbols_to_diagram",
    "t", "t1"
]

# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ìë™ ì‹¤í–‰


# â— ë¶„ì„ API ì—”ë“œí¬ì¸íŠ¸
@app.post("/analyze", response_model=AnalyzeResponse)  # sentenceë¥¼ ë°›ì•„ "sentence"ì™€ "diagramming" ë¦¬í„´
async def analyze(request: AnalyzeRequest):            # sentenceë¥¼ ë°›ì•„ ë‹¤ìŒ ì²˜ë¦¬ë¡œ ë„˜ê¹€
    init_memorys(request.sentence)                     # ì´ í•¨ìˆ˜ë¡œ ë©”ëª¨ë¦¬ ë‚´ìš© ì±„ì›€ ë˜ëŠ” ì´ˆê¸°í™”
    parsed = spacy_parsing_backgpt(request.sentence)               # GPTì˜ íŒŒì‹±ê²°ê³¼ë¥¼ parsedì— ì €ì¥
    memory["parsed"] = parsed
    apply_symbols(parsed)
    apply_subject_adverb_chunk_range_symbol(parsed)
    draw_dot_bridge_across_verb_group(parsed)
    return {"sentence": request.sentence,
            "diagramming": symbols_to_diagram(request.sentence),
            "verb_attribute": memory.get("verb_attribute", {}),
            "used_gpt": memory.get("used_gpt", False)  # âœ… ê²°ê³¼ í¬í•¨
    }


# â— spaCy íŒŒì‹± ê´€ë ¨
@app.post("/parse")
def parse_text(req: ParseRequest):
    doc = nlp(req.text)
    result = []

    for token in doc:
        morph = token.morph.to_dict()
        result.append({
            "idx": token.idx, "text": token.text, "pos": token.pos_, "tag": token.tag_,
            "dep": token.dep_, "head": token.head.text, "head_idx": token.head.idx,
            "tense": morph.get("Tense"), "aspect": morph.get("Aspect"), "form": morph.get("VerbForm"),
            "voice": morph.get("Voice"), "morph": morph, "lemma": token.lemma_,
            "is_stop": token.is_stop, "is_punct": token.is_punct, "is_alpha": token.is_alpha,
            "ent_type": token.ent_type_, "is_title": token.is_title,
            "children": [child.text for child in token.children]
        })

    return {"result": result}

# â— ì»¤ìŠ¤í…€ OpenAPI JSON ì œê³µ ì—”ë“œí¬ì¸íŠ¸
# FastAPIì—ì„œ custom-openapi.json ì—”ë“œí¬ì¸íŠ¸ë¥¼ ë§Œë“¤ì–´ì„œ GPTsì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•¨.
@app.get("/custom-openapi.json", include_in_schema=False)
async def serve_openapi():
    file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "openapi.json"))
    # file_path = os.path.join(os.path.dirname(__file__), "..", "openapi.json")
    return FileResponse(file_path, media_type="application/json")

# â— ì•„ë˜ ì—”ë“œí¬ì¸íŠ¸ëŠ” GET /ping ìš”ì²­ì— ëŒ€í•´ {"message": "pong"} ì‘ë‹µì„ ì¤€ë‹¤.
@app.get("/ping")
async def ping():
    return JSONResponse(content={"message": "pong"}, status_code=200)
##
