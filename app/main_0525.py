import os, json, re
import spacy
from fastapi import FastAPI
from fastapi.responses import JSONResponse, FileResponse  # renderì— 10ë¶„ ë‹¨ìœ„ Ping ë³´ë‚´ê¸°ë¥¼ ìœ„í•´ ì¶”ê°€
from pydantic import BaseModel
# ì•„ë˜ api_key= ê¹Œì§€ëŠ” .env íŒŒì¼ì—ì„œ OpenAIí‚¤ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° ê´€ë ¨ ë¶€ë¶„ 
from openai import OpenAI
from dotenv import load_dotenv

# â— í™˜ê²½ ì„¤ì •
load_dotenv()

api_key = os.getenv("API_KEY") or os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("âŒ OPENAI_API_KEY is not set in environment variables.")
client = OpenAI(api_key=api_key)

app = FastAPI()  # FastAPI() ê°ì²´ë¥¼ ìƒì„±í•´ì„œ ì´í›„ ë¼ìš°íŒ…ì— ì‚¬ìš©
nlp = spacy.load("en_core_web_sm")  # spaCy ê´€ë ¨ ì„¤ì •, (englihs_coreëª¨ë¸_webê¸°ë°˜_small)

# â— ë©”ëª¨ë¦¬ êµ¬ì¡°
memory = {
#    "characters": [],
    "symbols_by_level": {},
}

# â— ì‹¬ë³¼ ë§¤í•‘
role_to_symbol = {
    "verb": "â—‹",
    "object": "â–¡",
    "indirect object": "â–¡",
    "direct object": "â–¡",
    "prepositional object": "â–¡",
    "preposition": "â–½",
    "conjunction": "â—‡",
    "noun subject complement": "[",
    "adjective subject complement": "(",
    "noun object complement": "[",
    "adjective object complement": "("
}

SVOC_as_name = {
    "name", "appoint", "call", "elect", "consider", "make"
    # ì¼ë¶€ makeë„ í¬í•¨, He considered her a friend.
}

noSubjectComplementVerbs = {
    "live", "arrive", "go", "come", "sleep", "die", "run", "walk", "travel", "exist", "happen"
}

noObjectVerbs = {
    "die", "arrive", "exist", "go", "come", "vanish", "fall", "sleep", "occur"
}


# â— ìš”ì²­/ì‘ë‹µ ëª©ë¡
class AnalyzeRequest(BaseModel):   # ì‚¬ìš©ìê°€ ë³´ë‚¼ ìš”ì²­(sentence) ì •ì˜
    sentence: str

class AnalyzeResponse(BaseModel):  # ì‘ë‹µìœ¼ë¡œ ëŒë ¤ì¤„ ë°ì´í„°(sentence, diagramming) ì •ì˜
    sentence: str
    diagramming: str               # "     â—‹______â–¡__[         "

class ParseRequest(BaseModel):     # spaCy ê´€ë ¨ ì„¤ì •
    text: str


# rule ê¸°ë°˜ ë¶„ì„ ë¼ˆëŒ€ í•¨ìˆ˜ ì„ ì–¸
def rule_based_parse(tokens):
    result = []
    for t in tokens:
        t["children"] = [c["idx"] for c in tokens if c["head_idx"] == t["idx"]]

        # role ì¶”ë¡ 
        role = guess_role(t, tokens)
        if role:
            t["role"] = role  # combineì—ì„œ ì“°ì¼ ìˆ˜ ìˆìŒ

    # 'name'ê³¼ ê°™ì€ ë™ì‚¬ê°€ ìˆëŠ” SVOCêµ¬ì¡°ì—ì„œ ëª©ì ë³´ì–´ë¥¼ ì˜ëª» íƒœê¹…í•˜ëŠ” ê²ƒ ë³´ì • í•¨ìˆ˜
    result = tokens  # ê¸°ì¡´ tokensì„ ìˆ˜ì •í•˜ë©° ê³„ì† ì‚¬ìš©
    result = assign_svoc_complement_as_name(result)

    # âœ… combine ì¬ê³„ì‚°
    for t in result:
        combine = guess_combine(t, result)
        if combine:
            t["combine"] = combine

    # âœ… level ì¬ê³„ì‚°
    for t in result:
        t["level"] = guess_level(t, result)

    # âœ… ë³´ì–´ ê¸°ë°˜ object ë³µêµ¬ ìë™ ì ìš©
    result = repair_object_from_complement(result)        

    return result


# role ì¶”ë¡  í•¨ìˆ˜ìˆ˜
def guess_role(t, all_tokens=None):  # all_tokens ì¶”ê°€ í•„ìš”
    dep = t["dep"]
    pos = t["pos"]
    head_idx = t.get("head_idx")

    # âœ… Subject
    if dep in ["nsubj", "nsubjpass"]:
        return "subject"

    # âœ… Main Verb: beë™ì‚¬ í¬í•¨, ì¢…ì†ì ˆë„ ê³ ë ¤
    if pos in ["VERB", "AUX"] and dep in ["ROOT", "ccomp", "advcl", "acl"]:
        return "verb"

    # âœ… Indirect Object
    if dep in ["iobj", "dative"]:
        return "indirect object"

    # âœ… Direct Object (SVOO êµ¬ì¡° íŒë‹¨)
    if dep in ["dobj", "obj"]:
        if head_lemma := next((t["lemma"] for t in all_tokens if t["idx"] == head_idx), None):
            if head_lemma in noObjectVerbs:
                return None  # âŒ ëª©ì ì–´ ê¸ˆì§€ ë™ì‚¬ â†’ ë¬´ì‹œ

        # âœ… ê¸°ì¡´ object íŒë‹¨ ë¡œì§
        if all_tokens:
            for other in all_tokens:
                if other.get("dep") in ["iobj", "dative"] and other.get("head_idx") == head_idx:
                    return "direct object"
        return "object"

    # âœ… Preposition
    if dep == "prep":
        return "preposition"
    
    # âœ… Prepositional Object
    if dep == "pobj":
        return "prepositional object"

    # âœ… Conjunction or Clause Marker (ì ‘ì†ì‚¬)
    if dep in ["cc", "mark"]:
        return "conjunction"

    # âœ… Subject Complement (SVC êµ¬ì¡°)
    if dep in ["attr", "acomp"]:
        if head_lemma := next((t["lemma"] for t in all_tokens if t["idx"] == head_idx), None):
            if head_lemma in noSubjectComplementVerbs:
                return None  # âŒ ë³´ì–´ ë¶ˆê°€ ë™ì‚¬ â†’ ì°¨ë‹¨

        if pos in ["NOUN", "PROPN", "PRON"]:
            return "noun subject complement"
        elif pos == "ADJ":
            return "adjective subject complement"

    # âœ… Object Complement (ì •ìƒ ì¼€ì´ìŠ¤: dep=oprd, xcomp)
    if dep in ["oprd", "xcomp", "ccomp"]:
        if pos in ["NOUN", "PROPN", "PRON"]:
            return "noun object complement"
        elif pos == "ADJ":
            return "adjective object complement"

    # âœ… Object Complement (ë³´ì™„ ì¼€ì´ìŠ¤: dep=advmod, pos=ADJ, head=VERB, dobj ì¡´ì¬ ì‹œ)
    if dep == "advmod" and pos == "ADJ":
        for tok in all_tokens:
            if tok["idx"] == head_idx and tok["pos"] == "VERB":
                obj_exists = any(
                    c.get("head_idx") == tok["idx"] and c.get("dep") in ["dobj", "obj"]
                    for c in all_tokens
                )
                if obj_exists:
                    return "adjective object complement"

    # âœ… ê·¸ ì™¸ëŠ” DrawEnglish ë„ì‹ì—ì„œ ì‚¬ìš© ì•ˆ í•¨
    return None


def assign_svoc_complement_as_name(parsed):
    """
    SVOC êµ¬ì¡° ë™ì‚¬ë“¤(SVOC_as_name ì‚¬ì „ì— ë“±ë¡)ì˜ ëª©ì ë³´ì–´ê°€ spaCyì—ì„œ ì˜ëª» íƒœê¹…ëœ ê²½ìš° noun object complementë¡œ 1íšŒ ë³´ì •
    ë‹¨, object ì´í›„ì˜ ë‹¨ì–´ë§Œ ëŒ€ìƒìœ¼ë¡œ í•œë‹¤.
    """
    applied = False

    for i, token in enumerate(parsed):
        if token.get("lemma") in SVOC_as_name and token.get("pos") in ["VERB", "AUX"]:
            verb_idx = token["idx"]

            # object í™•ì¸
            obj = next((t for t in parsed if t.get("head_idx") == verb_idx and t.get("dep") in ["dobj", "obj"]), None)
            if not obj:
                continue

            obj_idx = obj["idx"]

            # ë³´ì–´ í›„ë³´ ì°¾ê¸° : objedt ë’¤ì— ìˆëŠ” ëª…ì‚¬ì‚¬
            for t in parsed:
                if applied:
                    break

                if (
                    t.get("idx") > obj_idx and  # âœ… object ì´í›„ì— ë“±ì¥í•œ ë‹¨ì–´ë§Œ
                    t.get("head_idx") == verb_idx and
                    t.get("dep") in ["nsubj", "nmod", "attr", "appos", "npadvmod", "ccomp"] and
                    t.get("pos") in ["NOUN", "PROPN"]
                ):
                    t["role"] = "noun object complement"
                    applied = True
                    break

    return parsed


# object complementê°€ ìˆëŠ”ë°, ì•ìª½ ëª©ì ì–´ë¥¼ nsubj(subject)ë¡œ ì˜ëª» íƒœê¹…í•˜ëŠ” ê²½ìš° ì˜ˆì™¸ì²˜ë¦¬
def repair_object_from_complement(parsed):
    for item in parsed:
        if item.get("role") in ["noun object complement", "adjective object complement"]:
            complement_children = item.get("children", [])
            for t in parsed:
                if t.get("dep") in ["nsubj", "compound"] and t.get("idx") in complement_children:
                    t["role"] = "object"
    return parsed


# combine ì¶”ë¡  í•¨ìˆ˜
def guess_combine(token, all_tokens):
    role = token.get("role")
    idx = token.get("idx")
    combine = []

    # âœ… Verb â†’ object / complement (SVO, SVC)
    if role == "verb":
        for t in all_tokens:
            if t.get("head_idx") == idx:
                r = t.get("role")
                if r in [
                    "object", 
                    "indirect object", 
                    "noun subject complement", 
                    "adjective subject complement"
                ]:
                    combine.append({"text": t["text"], "role": r})

    # âœ… Indirect object â†’ direct object (SVOO êµ¬ì¡°)
    if role == "indirect object":
        for t in all_tokens:
            if t.get("head_idx") == idx and t.get("role") == "object":
                combine.append({"text": t["text"], "role": "direct object"})

    # âœ… Object â†’ object complement (SVOC êµ¬ì¡°)
    if role == "object":
        for t in all_tokens:
            if t.get("head_idx") == idx and "object complement" in (t.get("role") or ""):
                combine.append({"text": t["text"], "role": t["role"]})

    # âœ… Preposition â†’ prepositional object
    if role == "preposition":
        for t in all_tokens:
            if t.get("head_idx") == idx and t.get("role") == "prepositional object":
                combine.append({"text": t["text"], "role": "prepositional object"})

    return combine if combine else None

# level ì¶”ë¡  í•¨ìˆ˜ìˆ˜
def guess_level(t, all_tokens):
    text = t["text"].lower()
    dep = t["dep"]
    pos = t["pos"]
    tag = t["tag"]

    # 1ï¸âƒ£ ì¢…ì†ì ‘ì†ì‚¬ (mark): that, because, if, although, when ë“±
    if dep == "mark" and text in ["that", "because", "if", "although", "when", "since", "while", "unless"]:
        return 0.5

    # 2ï¸âƒ£ ê´€ê³„ì ˆ: ê´€ê³„ëŒ€ëª…ì‚¬/í˜•ìš©ì‚¬/ë¶€ì‚¬ (relcl, acl ë“±)
    if dep in ["relcl", "acl"]:
        return 0.5

    # 3ï¸âƒ£ ê´€ê³„ì‚¬ (ê´€ê³„ëŒ€ëª…ì‚¬/ë¶€ì‚¬) ìì²´
    if text in ["who", "which", "that", "where", "when", "why", "whose", "whom"]:
        return 0.5

    # 4ï¸âƒ£ ë³µí•©ê´€ê³„ì‚¬ (whoever, whatever, whichever, wherever ë“±)
    if text in ["whoever", "whatever", "whichever", "wherever", "whomever", "whenever", "however"]:
        return 0.5

    # 5ï¸âƒ£ ì˜ë¬¸ì‚¬ (what, where, when ë“±) + ì˜ë¬¸ë¬¸ì´ ì•„ë‹Œ ë¬¸ì¥ ë‚´ë¶€ì— ìˆì„ ë•Œ
    if text in ["what", "who", "which", "where", "when", "why", "how"] and dep in ["nsubj", "dobj", "pobj"]:
        return 0.5

    # 6ï¸âƒ£ toë¶€ì •ì‚¬
    if text == "to":
        for child in t.get("children", []):
            for tok in all_tokens:
                if tok["text"] == child and tok["pos"] == "VERB":
                    return 0.5

    # 7ï¸âƒ£ í˜„ì¬ë¶„ì‚¬ (VBG), ê³¼ê±°ë¶„ì‚¬ (VBN)
    if tag.endswith("VBG") or tag.endswith("VBN"):
        return 0.5

    # ê¸°ë³¸ê°’
    return 0



# íŠ¸ë¦¬ê±° ë°œìƒì‹œ +0.5 ê·¸ ë‹¤ìŒ ë‹¨ì–´ level +1
def propagate_levels(parsed_tokens):
    final = []
    current_level = 0  # ê¸°ë³¸ì€ main level 0

    # ê° í† í°ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ level ë¶€ì—¬
    for i, token in enumerate(parsed_tokens):
        level = token.get("level", 0)

        # íŠ¸ë¦¬ê±°ê°€ ê°ì§€ë˜ë©´: 0.5 â†’ 1.5 â†’ 2.5 â†’ ...
        if isinstance(level, float) and level % 1 == 0.5:
            token["level"] = current_level + 0.5
            current_level += 1  # ë‹¤ìŒ ì ˆë¡œ ë„˜ì–´ê°€ë¯€ë¡œ +1
        else:
            token["level"] = current_level  # ì¼ë°˜ í† í°ì€ í˜„ì¬ level ìœ ì§€

        final.append(token)

    return final



# â— GPT í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜
def spacy_parsing_backgpt(sentence: str, force_gpt: bool = False):
    doc = nlp(sentence)

    prompt = f"""

"""
    # spaCyì—ì„œ í† í° ë°ì´í„° ì¶”ì¶œ
    tokens = []
    for token in doc:
        morph = token.morph.to_dict()
        tokens.append({
            "idx": token.idx, "text": token.text, "pos": token.pos_,"tag": token.tag_,
            "dep": token.dep_, "head": token.head.text, "head_idx": token.head.idx,
            "tense": morph.get("Tense"), "voice": morph.get("Voice"), "form": morph.get("VerbForm"),
            "morph": morph, "lemma": token.lemma_, "is_stop": token.is_stop,
            "is_punct": token.is_punct, "is_alpha": token.is_alpha, "ent_type": token.ent_type_,
            "is_title": token.is_title, "children": [child.text for child in token.children]
        })

    # ê·œì¹™ ê¸°ë°˜ íŒŒì‹±
    parsed = rule_based_parse(tokens)

    # ì¡°ê±´: ê·œì¹™ ê¸°ë°˜ ì‹¤íŒ¨í•˜ê±°ë‚˜, ê°•ì œë¡œ GPT ì‚¬ìš© ìš”ì²­
    if not parsed or force_gpt:
        prompt = gpt_parsing_withprompt(tokens)  # ì•„ë˜ 2ë‹¨ê³„ì—ì„œ ë§Œë“¤ ì˜ˆì •

        try:
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are an expert sentence analyzer."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=500
            )
            content = response.choices[0].message.content
            return json.loads(content)
        except Exception as e:
            print("[ERROR] GPT parsing failed:", e)
            print("[RAW CONTENT]", content if 'content' in locals() else '[No response]')
            return []

    # level ë³´ì •
    parsed = propagate_levels(parsed)

    return parsed

# GPT API Parsing(with í”„ë¡¬í”„íŠ¸)ì„ ì´ìš©í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
def gpt_parsing_withprompt(tokens: list) -> str:
    token_lines = []
    for t in tokens:
        token_lines.append(
            f"â— idx({t['idx']}), text({t['text']}), pos({t['pos']}), tag({t['tag']}), dep({t['dep']}), head({t['head']})"
        )
    token_block = "\n".join(token_lines)

    prompt = f"""
Given the following tokenized and POS-tagged English sentence, analyze its syntactic structure.

Token Info:
{token_block}

Return a JSON list with each token's role in the sentence.
Each item must have: idx, text, role, and optionally combine/level.

If unsure, return best-guess. Do not return explanations, just the JSON.
"""
    return prompt.strip()


# â— ì €ì¥ê³µê°„ ì´ˆê¸°í™”
def init_memorys (sentence: str):
#    memory["characters"] = list(sentence)        # charactersì— sentenceì˜ ê¸€ì í•œê¸€ìì”© ì±„ìš°ê¸°
    memory["symbols_by_level"] = {}  # ë¬¸ì¥ë§ˆë‹¤ ìƒˆë¡œ ì´ˆê¸°í™”
    memory["sentence_length"] = len(sentence)  # ë„ì‹ ê¸¸ì´ ì¶”ì ìš© (ì¤„ ê¸¸ì´ í†µì¼)


# â— symbols ë©”ëª¨ë¦¬ì— ì‹¬ë³¼ë“¤ ì €ì¥í•˜ê¸°
def apply_symbols(parsed):
    symbols_by_level = memory["symbols_by_level"]
    line_length = memory["sentence_length"]

    for item in parsed:
        idx = item.get("idx", -1)
        role = item.get("role", "").lower()
        level = item.get("level")

        if idx < 0 or level is None:
            continue

        # âœ… 0.5ì²˜ëŸ¼ ê²½ê³„ ë ˆë²¨ì€ ë‘ ì¤„ì— ì‹¬ë³¼ ì°ê¸°
        levels = [level]
        if isinstance(level, float) and level % 1 == 0.5:
            levels = [int(level), int(level) + 1]

        # 
        symbol = role_to_symbol.get(role)

        for lvl in levels:
            line = symbols_by_level.setdefault(lvl, [" " for _ in range(line_length)])
            if 0 <= idx < len(line) and line[idx] == " " and symbol:
                line[idx] = symbol


# â— memory["symbols"] ë‚´ìš©ì„ ì¶œë ¥í•˜ê¸° ìœ„í•´ ë§Œë“  í•¨ìˆ˜
def symbols_to_diagram(sentence: str):
    """
    Returns a visual diagram (string) of current symbols_by_level in memory.
    Includes:
    - index line
    - sentence line
    - symbol lines by level
    """
    output_lines = []

    line_length = memory["sentence_length"]

    # âœ… 1. ì¸ë±ìŠ¤ ì¤„
    index_line = ''.join(str(i % 10) for i in range(line_length))
    output_lines.append(index_line)

    # âœ… 2. ë¬¸ì¥ ì¤„
    output_lines.append(sentence)

    # âœ… 3. ì‹¬ë³¼ ì¤„ë“¤ (level ìˆœ ì •ë ¬)
    for level in sorted(memory["symbols_by_level"]):
        line = ''.join(memory["symbols_by_level"][level])
        output_lines.append(line)

    return '\n'.join(output_lines)



# â— ë””ë²„ê¹…ìš© í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def t(sentence: str):
    parsed = spacy_parsing_backgpt(sentence)
    parsed = propagate_levels(parsed)
    print("\nğŸ“Š Parsed Result:")
    for item in parsed:
        idx = item.get("idx")
        text = item.get("text")
        role = item.get("role")
        level = item.get("level")

        # combineì€ ë¦¬ìŠ¤íŠ¸ê±°ë‚˜ None
        combine = item.get("combine")
        if combine:
            combine_str = "[" + ', '.join(
                f"{c.get('text')}:{c.get('role')}" for c in combine
            ) + "]"
        else:
            combine_str = "None"

        print(f"â— idx({idx}), text({text}), role({role}), combine({combine_str}), level({level})")

    print(f"\nğŸ›  Diagram:")
    init_memorys(sentence)
    apply_symbols(parsed)
    print(symbols_to_diagram(sentence))

def t1(sentence: str):
    print(f"\nğŸ“˜ Sentence: {sentence}")
    doc = nlp(sentence)
    morph_data = []  # ì „ì²´ í† í° ë¦¬ìŠ¤íŠ¸ ì €ì¥

    # spaCyì—ì„œ full í† í° ì¶”ì¶œ
    for token in doc:
        morph = token.morph.to_dict()
        morph_data.append({
            "idx": token.idx, "text": token.text, "pos": token.pos_,
            "tag": token.tag_, "dep": token.dep_, "head": token.head.text,
            "head_idx": token.head.idx, "tense": morph.get("Tense"),
            "form": morph.get("VerbForm"), "voice": morph.get("Voice"),
            "morph": morph, "lemma": token.lemma_,
            "is_stop": token.is_stop, "is_punct": token.is_punct, "is_alpha": token.is_alpha,
            "ent_type": token.ent_type_, "is_title": token.is_title,
            "children": [child.text for child in token.children]
        })

    # êµ¬ì¡° ì¶”ë¡ 
    parsed = rule_based_parse(morph_data)
    parsed = propagate_levels(parsed)

    print("\nğŸ“Š Full Token Info with Annotations:")
    for token in morph_data:
        idx = token["idx"]
        text = token["text"]
        role = next((t.get("role") for t in parsed if t["idx"] == idx), None)
        combine = next((t.get("combine") for t in parsed if t["idx"] == idx), None)
        level = next((t.get("level") for t in parsed if t["idx"] == idx), None)

        combine_str = (
            "[" + ", ".join(f"{c['text']}:{c['role']}" for c in combine) + "]"
            if combine else "None"
        )

        child_texts = [t['text'] for t in parsed if t['idx'] in token.get('children', [])]

        print(f"â— idx({idx}), text({text}), role({role}), combine({combine_str}), level({level})")
        print(f"  POS({token['pos']}), DEP({token['dep']}), TAG({token['tag']}), HEAD({token['head']})")
        print(f"  lemma({token['lemma']}), is_stop({token['is_stop']}), is_punct({token['is_punct']}), is_title({token['is_title']})")
        print(f"  morph({token['morph']})")
        print(f"  children({child_texts}")
        print("")

    # ë„ì‹ ì¶œë ¥
    print("ğŸ›  Diagram:")
    init_memorys(sentence)
    apply_symbols(parsed)
    print(symbols_to_diagram(sentence))



# ì´ˆê°„ë‹¨ ì„ì‹œ í…ŒìŠ¤íŠ¸1 í•¨ìˆ˜



# â— ëª¨ë“ˆ ì™¸ë¶€ ì‚¬ìš©ì„ ìœ„í•œ export
__all__ = [
    "rule_based_parse",
    "guess_role",
    "guess_combine",
    "guess_level",
    "propagate_levels",
    "spacy_parsing_backgpt",
    "gpt_parsing_withprompt",
    "init_memorys",
    "apply_symbols",
    "symbols_to_diagram",
    "t", "t1"
]

# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ìë™ ì‹¤í–‰


# â— ë¶„ì„ API ì—”ë“œí¬ì¸íŠ¸
@app.post("/analyze", response_model=AnalyzeResponse)  # sentenceë¥¼ ë°›ì•„ "sentence"ì™€ "diagramming" ë¦¬í„´
async def analyze(request: AnalyzeRequest):            # sentenceë¥¼ ë°›ì•„ ë‹¤ìŒ ì²˜ë¦¬ë¡œ ë„˜ê¹€
    init_memorys(request.sentence)                     # ì´ í•¨ìˆ˜ë¡œ ë©”ëª¨ë¦¬ ë‚´ìš© ì±„ì›€ ë˜ëŠ” ì´ˆê¸°í™”
    parsed = spacy_parsing_backgpt(request.sentence)               # GPTì˜ íŒŒì‹±ê²°ê³¼ë¥¼ parsedì— ì €ì¥
    apply_symbols(parsed)                              # parsed ê²°ê³¼ì— ë”°ë¼ ì‹¬ë³¼ë“¤ì„ ë©”ëª¨ë¦¬ì— ì €ì¥ì¥
    return {"sentence": request.sentence,
            "diagramming": symbols_to_diagram(request.sentence)}

# â— spaCy íŒŒì‹± ê´€ë ¨
@app.post("/parse")
def parse_text(req: ParseRequest):
    doc = nlp(req.text)
    result = []

    for token in doc:
        morph = token.morph.to_dict()
        result.append({
            "idx": token.idx, "text": token.text, "pos": token.pos_, "tag": token.tag_,
            "dep": token.dep_, "head": token.head.text, "head_idx": token.head.idx,
            "tense": morph.get("Tense"), "form": morph.get("VerbForm"),
            "voice": morph.get("Voice"), "morph": morph, "lemma": token.lemma_,
            "is_stop": token.is_stop, "is_punct": token.is_punct, "is_alpha": token.is_alpha,
            "ent_type": token.ent_type_, "is_title": token.is_title,
            "children": [child.text for child in token.children]
        })

    return {"result": result}

# â— ì»¤ìŠ¤í…€ OpenAPI JSON ì œê³µ ì—”ë“œí¬ì¸íŠ¸
# FastAPIì—ì„œ custom-openapi.json ì—”ë“œí¬ì¸íŠ¸ë¥¼ ë§Œë“¤ì–´ì„œ GPTsì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•¨.
@app.get("/custom-openapi.json", include_in_schema=False)
async def serve_openapi():
    file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "openapi.json"))
    # file_path = os.path.join(os.path.dirname(__file__), "..", "openapi.json")
    return FileResponse(file_path, media_type="application/json")

# â— ì•„ë˜ ì—”ë“œí¬ì¸íŠ¸ëŠ” GET /ping ìš”ì²­ì— ëŒ€í•´ {"message": "pong"} ì‘ë‹µì„ ì¤€ë‹¤.
@app.get("/ping")
async def ping():
    return JSONResponse(content={"message": "pong"}, status_code=200)
#
